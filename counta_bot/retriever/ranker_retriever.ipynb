{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.elastic_db:Connecting to http://localhost:9200 \n",
      "INFO:utils.elastic_db:Connected to <Elasticsearch(['http://localhost:9200'])> \n",
      "INFO:utils.elastic_db:Connecting to http://localhost:9200 \n",
      "INFO:utils.elastic_db:Connected to <Elasticsearch(['http://localhost:9200'])> \n"
     ]
    }
   ],
   "source": [
    "# TODOs: Operate DB Class\n",
    "# TODOs: Use SQLite\n",
    "# TODOs: Implement BM25\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path[0] = str(Path(sys.path[0]).parent)\n",
    "\n",
    "from utils.elastic_db import ElasticDB\n",
    "\n",
    "# INIT DB OBJECT\n",
    "PORT = \"http://localhost:9200\"\n",
    "INDEX_NAME = \"news_cc\"\n",
    "\n",
    "news_db = ElasticDB(elastic_port=PORT, elastic_index=INDEX_NAME)\n",
    "wiki_db = ElasticDB(elastic_port=PORT, elastic_index=\"knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATASETS ###\n",
    "import json\n",
    "\n",
    "data = [json.loads(ln) for ln in open(\"../../data/train_cmv_cleaned.jsonl\")]\n",
    "topics = [json.loads(ln) for ln in open(\"../../data/claim_topics.jsonl\")]\n",
    "\n",
    "ex_retrieval = [json.loads(ln) for ln in open(\"../../data/wiki_doc_retrieved_from_op_train.jsonlist\")]\n",
    "ex_ranked = [json.loads(ln) for ln in open(\"../../data/selected_evidence.jsonl\")]\n",
    "\n",
    "conan = [json.loads(ln) for ln in open(\"../../data/CONAN.json\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3456"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tid': 't3_1hrvi8',\n",
       " 'retrieval_results': [{'retrieved_document_titles': ['Political abuse of psychiatry in the Soviet Union',\n",
       "    'Optical illusion',\n",
       "    'Positive illusions',\n",
       "    'Political positions of Noam Chomsky',\n",
       "    'Illusion'],\n",
       "   'query': 'illusions dissent a totalitarian state the usa these illusions characterized overt',\n",
       "   'sentence': 'The USA is or is becoming a totalitarian state characterized by illusions of choice , freedom , and democracy , as well as overt and physical suppresion of dissent whenever these illusions fail . '},\n",
       "  {'retrieved_document_titles': ['Police brutality',\n",
       "    'Police brutality in the United States',\n",
       "    'Riot shield',\n",
       "    'Black Panther Party',\n",
       "    '1967 Detroit riot'],\n",
       "   'query': 'riot shields police brutality cointelpro stuff',\n",
       "   'sentence': 'Spying on everybody , domestic drones , police brutality , COINTELPRO stuff , the walls of riot shields at NATO/WTO protests , the list goes on . '},\n",
       "  {'retrieved_document_titles': ['Holocaust denial',\n",
       "    'Climate change denial',\n",
       "    'Denial',\n",
       "    'Laws against Holocaust denial',\n",
       "    'Supportive housing'],\n",
       "   'query': 'denial the stuff supportive',\n",
       "   'sentence': 'What scares me is how many US citizens are in denial or are actually supportive of the stuff I just mentioned . '}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SAMPLE OUTPUT ###\n",
    "import random\n",
    "\n",
    "# TODOs: Replicate Output, Passage Evidence Retrieval\n",
    "# TODOs: Replicate Output, Passage Ranking\n",
    "\n",
    "_ = random.randint(0, 1000)\n",
    "ranked = ex_ranked[_]\n",
    "retireval = ex_retrieval[_]\n",
    "\n",
    "retireval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs: News Data\n",
    "# TODOs: Ranking\n",
    "# TODOs: Ranking, Cosine\n",
    "# TODOs: Research Evidence Retireval: Context Aware, Neural Retrieval\n",
    "# TODOs: Stance\n",
    "# TODOs: Target ADUs: Premises, Claims, discard non-ADUs, thus reducing noise over retreival \n",
    "\n",
    "# TODOs: Paralellise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cant remember the topic that spurred this discussion but a friend and I were debating whether manmade things were natural. He took the position that they are unnatural. He cited this definition by MerriamWebster existing in nature and not made or caused by people coming from nature as his basis for the distinction for natural vs. unnatural.However I respectfully disagree with his position and furthermore that definition of natural. People arise from nature. Humankinds capacity to create problemsolve analyze rationalize and build also come from natural processes. How are the things we create unnatural? It is only through natural occurrences that we have this ability why is it that we would give the credit of these things solely to man as opposed to nature? We are not separate from nature thus how can any of our actions or creations be unnatural? If we were somehow separate from nature I would understand the distinction between natural and manmade. However I think unnatural and manmade are not synonyms by any means. It seems to me that manmade things MUST be natural due to our being part of nature. I would love to hear your arguments and to have my view changed if I am mistaken in my logic somewhere along the line.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.155s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.068s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.003s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.066s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.049s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.004s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.065s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.153s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.055s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.003s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.126s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.011s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.069s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.004s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.029s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.004s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.067s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.003s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.070s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.068s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.006s]\n"
     ]
    }
   ],
   "source": [
    "from utils.keyphrase_extraction import extract_keyphrase\n",
    "# from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "_ = random.randint(0, 1000)\n",
    "\n",
    "arg = data[_][\"arguments\"]\n",
    "print(arg)\n",
    "\n",
    "def to_sentences(text):\n",
    "    return re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "\n",
    "sentences = to_sentences(arg)\n",
    "#sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', arg)\n",
    "\n",
    "queries = []\n",
    "results = []\n",
    "\n",
    "for sent in sentences:\n",
    "    if len(sent) < 5:\n",
    "        continue\n",
    "\n",
    "    kp = extract_keyphrase(sent, n_kp=5)\n",
    "    query = \", \".join(i for i in kp)\n",
    "    \n",
    "    # TODOs: Re-init DB with smaller passage size\n",
    "    titles = [i[\"_source\"][\"document\"][\"title\"] for i in news_db.search(query_=query, k=10)]\n",
    "    evidence = [i[\"_source\"][\"document\"][\"text\"] for i in news_db.search(query_=query, k=10)]\n",
    "\n",
    "    results.append({\n",
    "        \"argument_sentence\": sent, \n",
    "        \"retrieved_documents_titles\": titles,\n",
    "        \"query\": query,\n",
    "        \"retrieved_evidence\": evidence\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MERGE ###\n",
    "\n",
    "merged = []\n",
    "for counter_ev in results:\n",
    "    sentence = counter_ev[\"argument_sentence\"]\n",
    "    evidence = counter_ev[\"retrieved_evidence\"]\n",
    "    sent_kp = counter_ev[\"query\"]\n",
    "\n",
    "    merged_ev = \", \".join(retrieved for retrieved in evidence)\n",
    "\n",
    "    merged.append((sentence, sent_kp, merged_ev))\n",
    "\n",
    "#merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('He cited this definition by MerriamWebster existing in nature and not made or caused by people coming from nature as his basis for the distinction for natural vs.',\n",
       " 'nature, merriamwebster, definition, distinction, people')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PASSAGE RANKING : KEYWORD OVERLAP ###\n",
    "\n",
    "test = merged[2]\n",
    "test\n",
    "sent, kp, merge = test\n",
    "\n",
    "sent, kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Empty keyphrases. Perhaps the documents do not contain keyphrases that match the 'pos_pattern' parameter, only contain stop words, or you set the 'min_df'/'max_df' parameters too strict.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/retriever/ranker_retriever.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/retriever/ranker_retriever.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m merged_sentences:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/retriever/ranker_retriever.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mif\u001b[39;00m sent:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/retriever/ranker_retriever.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         kp \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m extract_keyphrase(sent, n_kp\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/retriever/ranker_retriever.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         scores\u001b[39m.\u001b[39mappend((kp, overlap_score(kp)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/retriever/ranker_retriever.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(sent_kps)\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/counta_bot/utils/keyphrase_extraction.py:9\u001b[0m, in \u001b[0;36mextract_keyphrase\u001b[0;34m(doc, n_gram, n_kp, use_mmr, use_maxsum)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_keyphrase\u001b[39m(doc, n_gram\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, n_kp\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, use_mmr\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFalse\u001b[39m\u001b[39m\"\u001b[39m, use_maxsum\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFalse\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m     \u001b[39m#kp = kb.extract_keywords(doc, keyphrase_ngram_range=(0, n_gram), stop_words=\"english\", diversity=0.2, vectorizer=KeyphraseCountVectorizer())\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     kp \u001b[39m=\u001b[39m kb\u001b[39m.\u001b[39;49mextract_keywords(doc, vectorizer\u001b[39m=\u001b[39;49mKeyphraseCountVectorizer())\n\u001b[1;32m     11\u001b[0m     \u001b[39m#kw_model.extract_keywords(doc, vectorizer=KeyphraseCountVectorizer())\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m [i[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m kp[\u001b[39m0\u001b[39m:n_kp]]\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/keybert/_model.py:134\u001b[0m, in \u001b[0;36mKeyBERT.extract_keywords\u001b[0;34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m# Extract potential words using a vectorizer / tokenizer\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m vectorizer:\n\u001b[0;32m--> 134\u001b[0m     count \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit(docs)\n\u001b[1;32m    135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/keyphrase_vectorizers/keyphrase_count_vectorizer.py:198\u001b[0m, in \u001b[0;36mKeyphraseCountVectorizer.fit\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_n_gram_length \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m([\u001b[39mlen\u001b[39m(keyphrase\u001b[39m.\u001b[39msplit()) \u001b[39mfor\u001b[39;00m keyphrase \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeyphrases])\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    199\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEmpty keyphrases. Perhaps the documents do not contain keyphrases that match the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpos_pattern\u001b[39m\u001b[39m'\u001b[39m\u001b[39m parameter, only contain stop words, or you set the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin_df\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_df\u001b[39m\u001b[39m'\u001b[39m\u001b[39m parameters too strict.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Empty keyphrases. Perhaps the documents do not contain keyphrases that match the 'pos_pattern' parameter, only contain stop words, or you set the 'min_df'/'max_df' parameters too strict."
     ]
    }
   ],
   "source": [
    "merged_sentences = to_sentences(merge)\n",
    "len(merged_sentences)\n",
    "\n",
    "sent_kps = \"\".join(i for i in kp)\n",
    "\n",
    "def count(string, substring):\n",
    "    n = len(substring)\n",
    "    cnt = 0\n",
    "    for i in range(len(string) - n):\n",
    "        if string[i:i+n] == substring:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "def overlap_score(count_kp):\n",
    "    score = 0\n",
    "\n",
    "    for i in count_kp:\n",
    "        score += count(sent_kps, count_kp)\n",
    "\n",
    "    return score\n",
    "\n",
    "scores = []\n",
    "for sent in merged_sentences:\n",
    "    if sent:\n",
    "        # TODOs: Try-Catch, Handel Errors\n",
    "        kp = [i for i in extract_keyphrase(sent, n_kp=3)]\n",
    "        scores.append((kp, overlap_score(kp)))\n",
    "\n",
    "print(sent_kps)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view, arguments, logic, line\n"
     ]
    }
   ],
   "source": [
    "print(sent_kp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ATTACKING PEMISES ###\n",
    "\n",
    "# from BERT_adu_classifier import predict\n",
    "\n",
    "# premises = []\n",
    "# for sent in sentences:\n",
    "#     prediction = predict(sent)\n",
    "    \n",
    "#     if prediction == \"premise\":\n",
    "#         premises.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/news_cc/_search [status:200 duration:0.367s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'argument_sentence': 'Fuck Islam and Fuck ISIS. We should repatriate all Muslims. They are all a bunch of violent criminals who do not share our values and never will. Without them Britain will be a better, safer place.',\n",
       "  'query': 'fuck isis repatriate muslims',\n",
       "  'retireved_documents': ['‘Assalamu Alaykom:’ peace be upon you',\n",
       "   'How We Got to Here: A Charlottesville Reading List',\n",
       "   'De-radicalization can work for former ISIS fighters',\n",
       "   'How to Get Assyrians to Stay',\n",
       "   'How to Get Christians to Stay']}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Argument Level\n",
    "\n",
    "# results = []\n",
    "\n",
    "# _ = 90\n",
    "# hate_ = sample_hate[0][\"conan\"][_][\"hateSpeech\"]\n",
    "\n",
    "# # queries = []\n",
    "# # sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', hate_)\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # TODOs: Refactor key_BERT as Class for multiple oututs\n",
    "# #kp_ = (exctract_keyphrase(hate_, n_gram=1, n_kp=1), exctract_keyphrase(hate_, n_gram=2, n_kp=1), exctract_keyphrase(hate_, n_gram=3, n_kp=1))\n",
    "\n",
    "# kp_ = (exctract_keyphrase(hate_, n_gram=5, n_kp=1))\n",
    "\n",
    "# query = \" ,\".join(i for i in kp_)\n",
    "# query\n",
    "\n",
    "# res = [i[\"_source\"][\"document\"][\"title\"] for i in news_db.search(query_=query, k=5)]\n",
    "\n",
    "# results.append({\n",
    "#     \"argument_sentence\": hate_, \n",
    "#     \"query\": query, \n",
    "#     \"retireved_documents\": res,\n",
    "\n",
    "# })\n",
    "\n",
    "# results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/knowledge/_search [status:200 duration:0.249s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'argument_sentence': 'Fuck Islam and Fuck ISIS. We should repatriate all Muslims. They are all a bunch of violent criminals who do not share our values and never will. Without them Britain will be a better, safer place.',\n",
       "  'query': 'fuck isis repatriate muslims',\n",
       "  'retireved_documents': ['Refuting ISIS',\n",
       "   'Refuting ISIS',\n",
       "   'Take Off (2017 film)',\n",
       "   'Terrorism in India',\n",
       "   'Tareq Kamleh']}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results = []\n",
    "# res = [i[\"_source\"][\"document\"][\"title\"] for i in wiki_db.search(query_=query, k=5)]\n",
    "\n",
    "# results.append({\n",
    "#     \"argument_sentence\": hate_, \n",
    "#     \"query\": query, \n",
    "#     \"retireved_documents\": res,\n",
    "\n",
    "# })\n",
    "\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'knowledge',\n",
       "  '_id': 'kbZa5YIBRqC1428DaQUk',\n",
       "  '_score': 33.672318,\n",
       "  '_ignored': ['document.text.keyword'],\n",
       "  '_source': {'document': {'id': '3408791',\n",
       "    'source': 'wikipedia',\n",
       "    'title': 'Ruling party',\n",
       "    'text': \"In parliamentary systems, the majority in the legislature also controls the executive branch of government, thus leaving no possibility of opposing parties concurrently occupying the executive and legislative branches of government. In other systems, such as in an American style presidential system, the party of the president does not necessarily also have a legislative majority. A 'ruling party' is also used to describe the party of one-party states, such as the Chinese Communist Party in the People's Republic of China. In his political manifesto 'The Green Book', the late Libyan leader Muammar al-Gaddafi attacked the ability of the ruling party, using it as a basis for his opposition to partisan politics. None\"}}},\n",
       " {'_index': 'knowledge',\n",
       "  '_id': 'JQ3H5YIBRqC1428Dnwum',\n",
       "  '_score': 33.24501,\n",
       "  '_ignored': ['document.text.keyword'],\n",
       "  '_source': {'document': {'id': '8622040',\n",
       "    'source': 'wikipedia',\n",
       "    'title': 'Fusion of powers',\n",
       "    'text': 'Fusion of powers is a feature of some parliamentary forms of government where different branches of government are intermingled, typically the executive and legislative branches. It is contrasted with the separation of powers found in presidential, semi-presidential and dualistic parliamentary forms of government, where the membership of the legislative and executive powers cannot overlap. Fusion of powers exists in many, if not a majority of, parliamentary democracies, and does so by design. However, in all modern democratic polities the judiciary does not possess legislative or executive powers. The system first arose as a result of political evolution in the United Kingdom over many centuries, as the powers of the monarch became constrained by Parliament.'}}},\n",
       " {'_index': 'knowledge',\n",
       "  '_id': 'OUcA5YIBRqC1428D6vvb',\n",
       "  '_score': 32.941406,\n",
       "  '_ignored': ['document.text.keyword'],\n",
       "  '_source': {'document': {'id': '18988566',\n",
       "    'source': 'wikipedia',\n",
       "    'title': 'Student governments in the United States',\n",
       "    'text': 'Structures. Many student governments are structured similarly to the federal government of the United States, consisting of distinct executive, legislative, and judicial branches. These structures often include elements which are not found in the federal government (e.g. legislative veto, programming branches, initiative, recall, referendum). Just like the federal government, these governments have the trappings of a presidential system, with a separation of powers between branches and a presidential veto. This is by far the most common type of structure, and is found in model student government constitutions and by-laws.'}}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #queries\n",
    "# query = \"the executive the presidential system legislative branches partisan unproductive\"\n",
    "# res = search_text(wiki_ev, query_=query, k=3)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruling party, Fusion of powers, Student governments in the United States\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "\n",
    "# print(\", \".join(re.sub(r'\\[\\[(?:[^|\\]]*\\|)?([^\\]]*)]]', \"\", i[\"_source\"][\"document\"][\"title\"]).strip(\"[]\") for i in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TEST SEARCH ###\n",
    "\n",
    "# def search_text(es, query_, k=5):\n",
    "#     results = es.search(\n",
    "#         index = es.elastic_index,\n",
    "#         query = {\n",
    "#             \"match\": {\n",
    "#                 \"document.text\": query_,\n",
    "#                 },\n",
    "#         },\n",
    "#         size=k)\n",
    "\n",
    "#     hits = results[\"hits\"][\"hits\"]\n",
    "#     doc_ids = [row['_source'][\"document\"][\"id\"] for row in hits]\n",
    "\n",
    "#     return hits\n",
    "\n",
    "# def search_topic(es, topic, k=5):\n",
    "#     results = es.search(\n",
    "#         index = es.elastic_index,\n",
    "#         body= {\n",
    "#             \"size\": k,\n",
    "#             \"query\": {\n",
    "#                 \"match\": {\n",
    "#                     \"document.title\": topic,\n",
    "#         }}})\n",
    "\n",
    "#     hits = results[\"hits\"][\"hits\"]\n",
    "#     doc_ids = [row['_source'][\"document\"][\"id\"] for row in hits]\n",
    "\n",
    "#     title = hits[0][\"_source\"][\"document\"][\"title\"]\n",
    "#     text = hits[0][\"_source\"][\"document\"][\"text\"]\n",
    "\n",
    "#     return {\n",
    "#         \"title\": title,\n",
    "#         \"text\": text\n",
    "#     }\n",
    "\n",
    "# query = \"government emails\"\n",
    "# text = search_text(wiki_ev, query_=query, k=10)\n",
    "\n",
    "# print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEARCH FUNCTION ###\n",
    "\n",
    "# TODOs: Implement as Class-DB object\n",
    "\n",
    "# def search_text(db, query_, k=5):\n",
    "#     results = db.search(\n",
    "#         index = es.elastic_index,\n",
    "#         query = {\n",
    "#             \"match\": {\n",
    "#                 \"document.text\": query_,\n",
    "#                 },\n",
    "#         },\n",
    "#         size=k)\n",
    "\n",
    "#     hits = results[\"hits\"][\"hits\"]\n",
    "#     doc_ids = [row['_source'][\"document\"][\"id\"] for row in hits]\n",
    "\n",
    "#     return hits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b747fec5972a5a28202124dfae2950631b4721a6e18efe99aaae23c73408484"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
