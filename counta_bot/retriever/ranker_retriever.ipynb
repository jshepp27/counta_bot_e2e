{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:counta_bot.utils.elastic_db:Connecting to http://localhost:9200 \n",
      "INFO:counta_bot.utils.elastic_db:Connected to <Elasticsearch(['http://localhost:9200'])> \n"
     ]
    }
   ],
   "source": [
    "# TODOs: Operate DB Class\n",
    "# TODOs: Use SQLite\n",
    "# TODOs: Implement BM25\n",
    "\n",
    "from counta_bot.utils.elastic_db import ElasticDB\n",
    "#from utils.elastic_db import ElasticDB\n",
    "\n",
    "# INIT DB OBJECT\n",
    "PORT = \"http://localhost:9200\"\n",
    "INDEX_NAME = \"news_cc\"\n",
    "\n",
    "# news_db = ElasticDB(elastic_port=PORT, elastic_index=INDEX_NAME)\n",
    "# wiki_db = ElasticDB(elastic_port=PORT, elastic_index=\"knowledge\")\n",
    "\n",
    "db = ElasticDB(elastic_port=PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### LOAD DATASETS ###\n",
    "import json\n",
    "\n",
    "args = [json.loads(ln) for ln in open(\"../../data/processed_train_cmv.jsonl\")]\n",
    "topics = [json.loads(ln) for ln in open(\"../../data/claim_topics.jsonl\")]\n",
    "ex_retrieval = [json.loads(ln) for ln in open(\"../../data/wiki_doc_retrieved_from_op_train.jsonlist\")]\n",
    "ex_ranked = [json.loads(ln) for ln in open(\"../../data/selected_evidence.jsonl\")]\n",
    "conan = [json.loads(ln) for ln in open(\"../../data/CONAN.json\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'tid': 't3_1pa6i9',\n 'retrieval_results': [{'retrieved_document_titles': ['Common minke whale',\n    'Whaling in Japan',\n    'Antarctic minke whale',\n    'Minke whale',\n    'Whaling in Iceland'],\n   'query': 'a huge outcry the japanese go hunting australia minke whales',\n   'sentence': 'Here in Australia we have a huge outcry whenever the Japanese go hunting for Minke whales in the southern ocean . '},\n  {'retrieved_document_titles': ['Sea Shepherd Conservation Society operations',\n    'Sea Shepherd Conservation Society',\n    'Whaling in Japan',\n    'Whale Wars',\n    'Anti-whaling'],\n   'query': 'clashes the japanese whalers the sea shepherd group',\n   'sentence': 'There are often clashes between the Sea Shepherd group and the Japanese whalers . '},\n  {'retrieved_document_titles': ['Ideograph (rhetoric)',\n    'Rhetoric',\n    'Philosophy of history',\n    'Modern liberalism in the United States',\n    'Racism'],\n   'query': 'political favour australia the political discourse decry',\n   'sentence': 'In the political discourse in Australia , politicians tend to decry the behaviour for political favour , as the majority of Australians are anti-whaling . '},\n  {'retrieved_document_titles': ['Whaling in Japan',\n    'International Whaling Commission',\n    'Whaling',\n    'Whaling in Iceland',\n    'Anti-whaling'],\n   'query': 'the moratorium commercial whaling recover',\n   'sentence': 'However the original reason for the moratorium on commercial whaling was a ten year ban to allow stocks to recover . '},\n  {'retrieved_document_titles': ['Whaling in Japan',\n    'International Whaling Commission',\n    'Whaling in Iceland',\n    'Anti-whaling',\n    'Whaling'],\n   'query': 'commercial whaling species endangered',\n   'sentence': \"I do n't agree with commercial whaling of species that are endangered or at risk . \"},\n  {'retrieved_document_titles': ['Common minke whale',\n    'Whaling in Japan',\n    'Antarctic minke whale',\n    'Minke whale',\n    'Whaling in Iceland'],\n   'query': 'other species minke whales the moratorium',\n   'sentence': 'For other species like Minke whales however , I see no reason to continue the moratorium to this day . '}]}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SAMPLE OUTPUT ###\n",
    "import random\n",
    "# TODOs: Replicate Output, Passage Evidence Retrieval\n",
    "# TODOs: Replicate Output, Passage Ranking\n",
    "\n",
    "_ = random.randint(0, 99)\n",
    "ranked = ex_ranked[_]\n",
    "retireval = ex_retrieval[_]\n",
    "\n",
    "retireval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODOs: News Data\n",
    "# TODOs: Ranking\n",
    "# TODOs: Ranking, Cosine\n",
    "# TODOs: Research Evidence Retireval: Context Aware, Neural Retrieval\n",
    "# TODOs: Stance\n",
    "# TODOs: Target ADUs: Premises, Claims, discard non-ADUs, thus reducing noise over retreival \n",
    "# TODOs: Paralellise\n",
    "\n",
    "# TERMS => Input Argument : args : arg\n",
    "# TERMS => Argument Discourse Units : adu : adu_prem : adu_claim\n",
    "# TERMS => Evidence : ev\n",
    "# TERMS => Counter Evidence : counta_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": "('Irony aside most people that post in are uneducated and changing their view on one point wont solve this overall problem',\n 'There are some instances yes where controversial topics are discussed here and people are actually interested in hearing the opposite side. I consider myself one of those people and was initially driven to this subreddit in the hope that I would meet other people like myself that were posting here to educate themselves. The type of man that reads a book that argues against a position he already holds in the hopes that he can understand the entire issue in a great light.But far and away this is not the case. Some of the posts on here include logical fallacies even in the titles yesterdays atheism post used a strawman fallacy in the title to misrepresent their own position to make it easier to defend in a religion vs. athiesm debate. The OP if you read that post was by and large not interested in changing his opinion and this is true of the vast majority of posts that come through here.And the problem is this it is a tremendous waste of time for every knowledgable person that posts answers in here. A lot of times the posts are not controversial they are simply the product of stupidity. There are a thousand places a million internet hits which one can browse to discover facts on the athiesmreligion debate and even the specific subtopic that was discussed yesterday. An educated person or rather a person seeking to educate himself would have simply utilized this vast body of resources rather than take time to create a new resource.Additionally the people that tend to post in this subreddit post on this subreddit have a certain mindset already even before submitting. Go ahead think of something you believe that is controversial and click Submit . Your mind will already be working to inhibit opposing opinions and you will be clinging harder than ever to the bulwarks of your argument. The fact of the matter is this it takes considerable effort to open ones mind even on things that are not of great consequence. And most people are simply unable to accomplish this.Thanks for your replies. I intend to reply to each response to my original post albeit maybe not today and I hope to award many deltas for your efforts.')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SUBJECT ARG ###\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Note: 340; Gender Equality\n",
    "# Note: 991; Abortion\n",
    "\n",
    "print(_)\n",
    "\n",
    "claim = args[_][\"title\"]\n",
    "arg = args[_][\"argument\"]\n",
    "claim, arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['hello, my name is Josh! How are you doing today?',\n \"I'm curious ... will this line seperate?\",\n \"I'm not so sure Dr. Evil\"]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### NLP FUNCTIONS ###\n",
    "import re\n",
    "\n",
    "def sentences_segment(doc):\n",
    "    return [i for i in re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', doc)]\n",
    "\n",
    "def tokeniser(doc):\n",
    "    return re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", doc)\n",
    "\n",
    "# Test Statements\n",
    "tokeniser(\"hello, my name is Josh!\")\n",
    "sentences_segment(\"hello, my name is Josh! How are you doing today? I'm curious ... will this line seperate? I'm not so sure Dr. Evil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.064s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.021s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.019s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.029s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.019s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.010s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.020s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.014s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.030s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.018s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.069s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.065s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.017s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.011s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.011s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.013s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.052s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.039s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.025s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.021s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.036s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.036s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.033s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.017s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.014s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.014s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.011s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.037s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.033s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.049s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.044s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.046s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.044s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.013s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.010s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.023s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.017s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.029s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.010s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.010s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.019s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.010s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.018s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.014s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.010s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.028s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.023s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.016s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.014s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.030s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.021s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.035s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.029s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.047s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.043s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.025s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.016s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.005s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.013s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.011s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.033s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.029s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.017s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.014s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.053s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.046s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.015s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.058s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.051s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.039s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.036s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.010s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.054s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.058s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.011s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.069s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.063s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.011s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.010s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.010s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.050s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.041s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.005s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.005s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.018s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.015s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.011s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.018s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.015s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.024s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.020s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.021s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.018s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.015s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.011s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.005s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.010s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.024s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.022s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.022s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.018s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.005s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.004s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.026s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.022s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.074s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.071s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.064s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.060s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.013s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.011s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.025s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.020s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.062s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.057s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.014s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.042s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.036s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.015s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.014s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.027s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.023s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.030s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.029s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.014s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.047s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.046s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.010s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.015s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.013s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.024s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.021s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.010s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.014s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.013s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.024s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.022s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.050s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.044s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.020s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.019s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.023s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.020s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.008s]\n"
     ]
    }
   ],
   "source": [
    "from counta_bot.detection.stance_classifier import sentence_stance, compare_stance\n",
    "\n",
    "# TODOs: Implement Keyphrase Extraction and Tokenisation as a pre-processing step; Be mindful of interactive pipeline mode\n",
    "# TODOs: Implement Semantic Search: https://www.elastic.co/blog/text-similarity-search-with-vectors-in-elasticsearch \n",
    "# TODOs: Fix Sentencizer\n",
    "# TODOs: Tokeniser Function\n",
    "# TODOs: Domain Restrict - polarising social and political debate (Class labelling); Note: currently, open-domain. Score Highly Polarised Discussions.\n",
    "# TODOs: Bag of Topics, Concepts for Topic Labelling\n",
    "# TODOs: News, Political, Sociology and 'Good', 'Positive' counter-evidence Knowledge Base.\n",
    "# TODOs: Consider parsing and normalising knowledge; extracting core ADU arguments, premises, evidence and claims.\n",
    "# TODOs: Sentence Segment Function\n",
    "# TODOs: Semantic Retrieval ** \n",
    "# TODOs: Fix Sentence parsing\n",
    "# TODOs: Parameterise Index-DB in use\n",
    "# TODOs: Implement as a Class\n",
    "# TODOs: Implement Logging\n",
    "# TODOs: One Argument Loop (1 x # ADUs)\n",
    "# TODOs: Filter ADUs; Strong and disputable arguments (Premise rakning, NLI, Argument Similarity (counter stance))\n",
    "# TODOs: Reduce the size of the Argument using extractive summarisation\n",
    "# TODOs: Reduce the size of the Argument Targeting Premises Only\n",
    "# TODOs: Experiement with Query\n",
    "# TODOs: Write to Database\n",
    "\n",
    "### RETRIEVER ###\n",
    "db = db\n",
    "queries = []\n",
    "retrieved_ev = []\n",
    "\n",
    "def retrieved_evidence(arg):\n",
    "    ad_units = sentences_segment(arg[\"argument\"])\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for adu in ad_units:\n",
    "        toks = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", adu)\n",
    "\n",
    "        if len(toks) <= 8:\n",
    "            continue\n",
    "    \n",
    "        kp = arg[\"keyphrase\"]\n",
    "\n",
    "        if kp:\n",
    "            query = \", \".join(i for i in kp)\n",
    "\n",
    "            titles = [i[\"_source\"][\"document\"][\"title\"] for i in db.search(query_=query, k=10)]\n",
    "            evidence = [i[\"_source\"][\"document\"][\"text\"] for i in db.search(query_=query, k=10)]\n",
    "\n",
    "            results.append({\n",
    "                \"tid\": arg[\"id\"],\n",
    "                \"argument_discourse_unit\": adu, \n",
    "                \"retrieved_documents_titles\": titles,\n",
    "                \"query\": query,\n",
    "                \"adu_keyphrases\": [i for i in kp],\n",
    "                \"adu_stance\": sentence_stance(adu, kp),\n",
    "                \"retrieved_evidence\": evidence,\n",
    "                \"merged_evidence\": \", \".join(ln for ln in evidence)\n",
    "                })\n",
    "\n",
    "        return results\n",
    "\n",
    "retrieved_ev = []\n",
    "\n",
    "# Note: Index into json.loads(object)[0] to extract dictionary object\n",
    "for arg in args[0:100]:\n",
    "    retrieved_ev.append(retrieved_evidence(arg[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'CON'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adu = 'I cant remember the topic that spurred this discussion but a friend and I were debating whether manmade things were natural.'\n",
    "ev_unit = 'In this essay, Mill argues the idea that the morality of an action can be judged by whether it is natural or unnatural.'\n",
    "target = 'natural things'\n",
    "\n",
    "stance = compare_stance(ev_unit, target)\n",
    "stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:05<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.6755211353302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from yake import KeywordExtractor\n",
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "from summa import keywords\n",
    "from tqdm import tqdm\n",
    "\n",
    "### PASSAGE RANKING; KEYWORD OVERLAP ###\n",
    "kw_extractor = KeywordExtractor(lan=\"en\", n=3, top=5)\n",
    "\n",
    "# TODOs: For each ADU, Rank Merged Evidence using Keyword Overlap and Filter for Contrasting Stance\n",
    "# TODOs: Handel Multiple Keywords\n",
    "\n",
    "def overlap_score(evidence_kp, adu_kp):\n",
    "    score = 0\n",
    "    # TODOs: Robust 'None' Handelling\n",
    "    if adu_kp == None:\n",
    "        return score\n",
    "    # Split Keyphrase into components, scoring partial units as overlap\n",
    "    else:\n",
    "        for i in evidence_kp:\n",
    "            for j in i.split():\n",
    "                # Ensure string value, to enact .find\n",
    "                if \", \".join([i for i in adu_kp]).find(j) != -1: score += 1\n",
    "                \n",
    "                else: continue\n",
    "    \n",
    "    return score\n",
    "\n",
    "def calculate_overlap(merged_ev, adu_kp):\n",
    "\n",
    "    for ev_unit in sentences_segment(merged_ev):\n",
    "        toks = tokeniser(ev_unit)\n",
    "        kp_overlap = 0\n",
    "        \n",
    "        if len(toks) <= 8: continue\n",
    "\n",
    "        #ev_unit_kp = [i for i in keywords.keywords(ev_unit).split(\"\\n\")]\n",
    "        ev_unit_kp = [i[0] for i in kw_extractor.extract_keywords(ev_unit)]\n",
    "\n",
    "        if ev_unit_kp:\n",
    "            kp_overlap = overlap_score(evidence_kp=ev_unit_kp, adu_kp=adu_kp)\n",
    "        \n",
    "        else: ev_unit_kp = None\n",
    "        yield ev_unit, ev_unit_kp, kp_overlap\n",
    "\n",
    "# pool = Pool(8)\n",
    "### RANK PASSAGES ###\n",
    "def score_passages(ev_):   \n",
    "    adu = ev_[0][\"argument_discourse_unit\"]\n",
    "    adu_stance = ev_[0][\"adu_stance\"]\n",
    "    merged_ev = ev_[0][\"merged_evidence\"] \n",
    "    adu_kp = ev_[0][\"adu_keyphrases\"]\n",
    "    \n",
    "    ### CALCULATE OVERLAP ###\n",
    "    for ev_unit, ev_unit_kp, kp_overlap in calculate_overlap(merged_ev, adu_kp):\n",
    "        target = adu_kp[0]\n",
    "\n",
    "        compared_stace = compare_stance(ev_unit, target)\n",
    "        if compared_stace != adu_stance:\n",
    "            yield {\n",
    "                \"adu\": adu,\n",
    "                \"adu_kp\": adu_kp,\n",
    "                \"evidence_unit\": ev_unit,\n",
    "                \"evidence_kps\": ev_unit_kp,\n",
    "                \"overlap\": kp_overlap,\n",
    "                \"evidence_stance\": compare_stance(ev_unit, target),\n",
    "                \"adu_stance\": adu_stance\n",
    "            }\n",
    "\n",
    "        else: continue\n",
    "\n",
    "### SCORED EVIDENCE ###\n",
    "def score_evidence(retrieved_evidence):\n",
    "    for ev_ in retrieved_ev:\n",
    "        yield [i for i in score_passages(ev_)]\n",
    "\n",
    "### RANKED EVIDENCE ###\n",
    "def rank_filter_counter_evidence(retireved_evidence, k=3):\n",
    "    with tqdm(total=(len(retrieved_ev))) as pbar:\n",
    "        for i in score_evidence(retrieved_ev):\n",
    "            yield sorted(i, key=lambda y: y[\"overlap\"], reverse=True)[0:k]\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "\n",
    "### SELECT TOP-K COUNTER-EVIDENCE ###\n",
    "tic = time.time()\n",
    "ranked_sorted_evidence = [i for i in rank_filter_counter_evidence(retrieved_ev)]\n",
    "ranked_sorted_evidence\n",
    "toc = time.time()\n",
    "\n",
    "print(toc - tic)\n",
    "# TIME 1:20M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "fout = open(\"../../data/rr_counter_evidence.jsonl\", \"w\")\n",
    "args = [json.loads(ln) for ln in open(\"../../data/processed_train_cmv.jsonl\")]\n",
    "\n",
    "for args, adus in zip(args, ranked_sorted_evidence):\n",
    "    for arg in args:\n",
    "        adus_ = []\n",
    "        for unit in adus:\n",
    "            adus_.append({\n",
    "                \"adu\": unit[\"adu\"],\n",
    "                \"adu_kp\": unit[\"adu_kp\"],\n",
    "                \"evidence_unit\": unit[\"evidence_unit\"],\n",
    "                \"evidence_kps\": unit[\"evidence_kps\"],\n",
    "                \"overlap\": unit[\"overlap\"],\n",
    "                \"evidence_stance\": unit[\"evidence_stance\"],\n",
    "                \"adu_stance\": unit[\"adu_stance\"],\n",
    "            })\n",
    "\n",
    "        fout.write(json.dumps([{\n",
    "            \"ids\": arg[\"id\"],\n",
    "            \"rank_retrieved_filered\": adus_\n",
    "        }]))\n",
    "\n",
    "        fout.write(\"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "#args = [json.loads(ln) for ln in open(\"../../data/train_cmv_cleaned.jsonl\")]\n",
    "#args = [json.loads(ln) for ln in open(\"../../data/train_cmv_cleaned.jsonl\")]\n",
    "args = [json.loads(ln) for ln in open(\"../../data/gpt_ft_kg/arguments_counters_evidence.jsonl\", \"r\")]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'Look at the definition you provided if we remove the exclusion of things which humans createSo essentially by this definition natural things are things that exist which is frankly rather meaningless. If one wanted to discuss the results of human activity we would then have to make up a new word which could be redefined by the same argument. The whole point of the word is to exclude human activity. If you remove that aspect it simply ceases to have meaning.'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args[0][\"counters\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # TODOs: Speed-up, Parrelleise, Yield\n",
    "# def overlap_score(evidence_kp, adu_kp):\n",
    "#     score = 0\n",
    "\n",
    "#     # Split Keyphrase into components, scoring partial units as overlap\n",
    "#     for i in evidence_kp:\n",
    "#         for j in i.split():\n",
    "#             # Ensure string value, to enact .find\n",
    "#             if \" \".join(adu_kp).find(j) != -1: score += 1\n",
    "\n",
    "#             else: continue\n",
    "\n",
    "#     return score\n",
    "\n",
    "# ev_units = evidence\n",
    "# adu_kp = extract_keyphrase(adu)\n",
    "\n",
    "# adu_ev_overlap = []\n",
    "\n",
    "# kp_1 = ['sex', 'relationship', 'opportunity']\n",
    "# kp_2 = ['better sex']\n",
    "\n",
    "# overlap_score(kp_2, kp_1)\n",
    "\n",
    "# for ev_unit in evidence:\n",
    "#     #print(ev_unit)\n",
    "#     toks = tokeniser(ev_unit)\n",
    "\n",
    "#     # Exprimental Value\n",
    "#     if len(toks) <= 8:\n",
    "#         continue\n",
    "\n",
    "#     ev_unit_kp = extract_keyphrase(ev_unit)\n",
    "#     kp_overlap = overlap_score(evidence_kp=ev_unit_kp, adu_kp=adu_kp)\n",
    "\n",
    "#     adu_ev_overlap.append({\n",
    "#         \"adu\": adu,\n",
    "#         \"adu_kp\": adu_kp,\n",
    "#         \"ev_unit\": ev_unit,\n",
    "#         \"ev_unit_kp\": ev_unit_kp,\n",
    "#         \"kp_overlap\": kp_overlap\n",
    "\n",
    "#         })\n",
    "\n",
    "# adu_ev_overlap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# ### OVERLAP RANKED EVIDENCE ###\n",
    "\n",
    "# adu_ev_overlap.sort(key=lambda y: y[\"kp_overlap\"], reverse=True)\n",
    "# adu_ev_overlap\n",
    "\n",
    "# ### FILTER IRRELEVANT EVIDENCE ###\n",
    "# overlapping = [i for i in adu_ev_overlap if i[\"kp_overlap\"] !=0]\n",
    "\n",
    "# len(adu_ev_overlap), len(overlapping)\n",
    "# overlapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# ### ASSERT SAME STANCE ###\n",
    "# from detection.stance_classifier import sentence_stance, compare_stance\n",
    "#\n",
    "# # TODOs: Ensure KPs Extracts are constrained to 1 unit\n",
    "# opposing_stance = []\n",
    "# for i in overlapping:\n",
    "#     adu = i[\"adu\"]\n",
    "#     target = \" \".join(i for i in i[\"adu_kp\"])\n",
    "#     ev_unit = i[\"ev_unit\"]\n",
    "#\n",
    "#     ev_stance = compare_stance(ev_unit, ev_unit, target)\n",
    "#     adu_stance = sentence_stance(adu, target)\n",
    "#\n",
    "#     if ev_stance != adu_stance:\n",
    "#         opposing_stance.append((ev_unit, ev_stance, adu_stance))\n",
    "#\n",
    "#     else: continue\n",
    "#\n",
    "# opposing_stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### RANKING ###\n",
    "\n",
    "# TODOs: Speed-up, Parrelleise, Yield\n",
    "# ev_units = evidence\n",
    "# adu_kp = extract_keyphrase(adu)\n",
    "\n",
    "# adu_ev_overlap = []\n",
    "\n",
    "# kp_1 = ['sex', 'relationship', 'opportunity'] \n",
    "# kp_2 = ['better sex']\n",
    "\n",
    "# overlap_score(kp_2, kp_1)\n",
    "\n",
    "# for ev_unit in evidence:\n",
    "#     #print(ev_unit)\n",
    "#     toks = tokeniser(ev_unit)\n",
    "\n",
    "#     # Exprimental Value\n",
    "#     if len(toks) <= 8:\n",
    "#         continue\n",
    "    \n",
    "#     ev_unit_kp = extract_keyphrase(ev_unit)\n",
    "#     kp_overlap = overlap_score(evidence_kp=ev_unit_kp, adu_kp=adu_kp)\n",
    "    \n",
    "#     adu_ev_overlap.append({\n",
    "#         \"adu\": adu, \n",
    "#         \"adu_kp\": adu_kp,\n",
    "#         \"ev_unit\": ev_unit,\n",
    "#         \"ev_unit_kp\": ev_unit_kp, \n",
    "#         \"kp_overlap\": kp_overlap\n",
    "        \n",
    "#         })\n",
    "        \n",
    "# adu_ev_overlap\n",
    "\n",
    "\n",
    "#rank_passages(retrieved_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.matcher import PhraseMatcher\n",
    "# from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# # TODOs: Package as a Module\n",
    "# # TODOs: Handle Negation (Polarity shifters)\n",
    "# # TODOs: Review Unsuperived Approach; Consider adveanced patterns and common-sence knowledge\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# sentence = \"I hate abortion rights. Abortions should be banned.\"\n",
    "# sentence_2 = \"I like abortion rights. I belive we should keep them.\"\n",
    "# sentence_3 = \"I hate tennis. People should play tennis more often\"\n",
    "\n",
    "# ### STANCE SCORING ###\n",
    "\n",
    "# # TODOs: https://www.cs.uic.edu/~liub/FBS/opinion-mining-final-WSDM.pdf \n",
    "# # TODOs: Pattern based Negation\n",
    "# # TODOs: Semantic Orientation of an opinion (Claim)\n",
    "# # TODOs:Group synonyms of 'features', 'targets'\n",
    "\n",
    "# phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# ### SENTIMENT LEXICONS ###\n",
    "# pos = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/positive_lex.txt\")]\n",
    "# neg = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/negative_lex.txt\")]\n",
    "# polarity_shifters = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/shifter_lexicon.txt\")]\n",
    "\n",
    "# ### STANCE: ASPECT-SEMANTIC ORIENTATION ###\n",
    "# def extract_aspect(sentence, n_gram):\n",
    "#     aspects = extract_keyphrase(str(sentence))[0]\n",
    "\n",
    "#     return nlp(aspects)\n",
    "\n",
    "# def index_aspect(phrase, aspect, sentence):    \n",
    "#     patterns = [nlp(aspect)]\n",
    "#     phrase_matcher.add(phrase, None, *patterns)\n",
    "\n",
    "#     start = 0\n",
    "#     stop = 0\n",
    "\n",
    "#     matched_phrases = phrase_matcher(sentence)\n",
    "#     for i in matched_phrases:\n",
    "#         _, start, stop = i\n",
    "        \n",
    "#     return start, stop\n",
    "\n",
    "# # TODOs: Implement Polarity Shifters, Simple\n",
    "# # TODOs: Implement Polarity Shifters, Complex, Verb Patterns\n",
    "# def stance_score(start, stop, sentence):\n",
    "#     pos_score = 0.0\n",
    "#     neg_score = 0.0\n",
    "\n",
    "#     score = 0\n",
    "#     for idx, tok in enumerate(sentence):\n",
    "#         if idx == start or idx == stop:\n",
    "#             continue\n",
    "\n",
    "#         # TODOs: Implement Polarity Shift\n",
    "#         # TODOs: Experiement with descriptive term + keyphrase aspects\n",
    "#         # TODOs: ABSA https://www.kaggle.com/code/phiitm/aspect-based-sentiment-analysis\n",
    "#         # Use external libaray: Textblob\n",
    "        \n",
    "#         k = 8\n",
    "#         # Negation Rules\n",
    "#         shifted_tok = None\n",
    "#         shifted_toks = []\n",
    "\n",
    "#         if (tok.dep_ == \"neg\") or (tok.dep_ in polarity_shifters):\n",
    "#             #Shift to Negative\n",
    "#             if idx <= k:\n",
    "#                 if idx < start: neg_score += 1/(start - idx)\n",
    "#                 else: neg_score += 1/(idx - stop)**0.5\n",
    "\n",
    "#             if shifted_tok != None and shifted_tok in neg:\n",
    "#                 print(shifted_tok.text)\n",
    "#                 # Shift to Positive\n",
    "#                 if idx < start: pos_score += 1/(start - idx)\n",
    "#                 elif idx > start: pos_score += 1/(idx - stop)**0.5\n",
    "#                 else: continue\n",
    "\n",
    "#         # Aspect Sentement Orientation\n",
    "#         if tok.text in pos:\n",
    "#             if tok in shifted_toks:\n",
    "#                 continue\n",
    "            \n",
    "#             if idx < start: pos_score += 1/(start - idx)\n",
    "#             else: pos_score += 1/(idx - stop)**0.5\n",
    "\n",
    "#         if tok.text in neg:\n",
    "#             if tok in shifted_toks:\n",
    "#                 continue\n",
    "\n",
    "#             if idx <= start: neg_score += 1/(start - idx)\n",
    "#             else: neg_score += 1/(idx - stop)**0.5\n",
    "    \n",
    "#     score = pos_score - neg_score /(pos_score + neg_score + 1)\n",
    "\n",
    "#     return score\n",
    "\n",
    "# def overlap_score(evidence_kp, adu_kp):\n",
    "#     score = 0\n",
    "    \n",
    "#     # Split Keyphrase into components, scoring partial units as overlap\n",
    "#     for i in evidence_kp:\n",
    "#         for j in i.split():\n",
    "#             # Ensure string value, to enact .find\n",
    "#             if \" \".join(adu_kp).find(j) != -1: \n",
    "#                 score += 1\n",
    "#                 token = j\n",
    "            \n",
    "#             else: continue\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# def get_overlapping_token(evidence_kp, adu_kp):\n",
    "#     for i in evidence_kp:\n",
    "#         overlap_tokens = []\n",
    "#         for j in i.split():\n",
    "#             if \" \".join(adu_kp).find(j) != -1: \n",
    "#                 overlap_tokens.append(j) \n",
    "            \n",
    "#         return \" \".join(i for i in overlap_tokens)\n",
    "\n",
    "# def sentence_stance(sentence, aspect):\n",
    "#     sentence = nlp(sentence)\n",
    "\n",
    "#     start, stop = index_aspect(\"aspects\", aspect, sentence)\n",
    "#     score = stance_score(start, stop, sentence)\n",
    "\n",
    "#     # Add Neutral\n",
    "#     #stance = {\"claim\": sentence, \"stance\": \"PRO\", \"aspect\": aspect} if score > 0 else {\"claim\": sentence, \"stance\": \"CON\", \"aspect\": aspect}\n",
    "    \n",
    "#     return \"PRO\" if score > 0 else \"CON\"\n",
    "\n",
    "# def fuzzy_match(target, evidence_unit):\n",
    "\n",
    "#     overlapping_aspect = process.extractOne(target, ev.split())[0]\n",
    "#     score = overlapping_aspect[1]\n",
    "\n",
    "#     overlapping_aspect = nlp(re.sub(r'[^\\w]', ' ', overlapping_aspect))\n",
    "\n",
    "#     return overlapping_aspect, score\n",
    "\n",
    "# def compare_stance(ev_unit, evidence_aspect, adu_target):\n",
    "#     # Note: Already identified mathcing or partially matching Aspects. \n",
    "\n",
    "#     # Get the overlapping evidence aspect-target.\n",
    "#     overlapping_target, score = fuzzy_match(target=adu_aspect, evidence_unit=ev)\n",
    "    \n",
    "#     # Get position of the overlapping_target\n",
    "#     start, stop = index_aspect(\"OVERLAP\", nlp(overlapping_target), nlp(ev_unit))\n",
    "\n",
    "#     # Assert Stance towards evidence aspect\n",
    "#     score = stance_score(start, stop, nlp(ev_unit))\n",
    "    \n",
    "#     return \"PRO\" if score > 0 else \"CON\"\n",
    "\n",
    "# ev = \"These simple ideas and techniques could help both you and your lover enjoy sex. 1 / 10 Getty Images/Caiaimage Think beyond the thrust.\"\n",
    "# ev_aspect = \"sex\", \"relationship\", \"opportunity\"\n",
    "\n",
    "# adu = 'Hello! Let me preface by saying I dont believe there is a better sex.'\n",
    "# adu_aspect = \"better sex\"\n",
    "\n",
    "# print(sentence_stance(\"The mutual trust and understanding you share with your partner will lead to better sex, but that's not the only reason sex can be better when you're not in a relationship.\", adu_aspect))\n",
    "# print(compare_stance(ev, ev_aspect, adu_aspect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from spacy.matcher import DependencyMatcher, Matcher\n",
    "# matcher = Matcher(vocab=nlp.vocab)\n",
    "# matcher\n",
    "\n",
    "# # Matching Rule: Pronouns with Verbs that follow them\n",
    "# aspect = \"better sex\"\n",
    "# patterns = [\n",
    "#     [{\"DEP\": \"neg\"}, {\"LOWER\": aspect}],\n",
    "#     [{\"DEP\": \"neg\"}, {\"POS\": \"ADJ\"}, {\"LOWER\": aspect}],\n",
    "#     [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}, {\"LOWER\": aspect}],\n",
    "#     [{\"LOWER\": aspect.lower()}]\n",
    "# ]\n",
    "\n",
    "# test = nlp(\"Hello! Let me preface by saying I dont believe there is a not better sex.\")\n",
    "# test_2 = nlp(\"These simple ideas and techniques could help both you and your lover enjoy better sex.\")\n",
    "\n",
    "# matcher.add(\"test\", patterns=patterns)\n",
    "# result = matcher(test_2, as_spans=True)\n",
    "\n",
    "# result\n",
    "\n",
    "# # for tok in test:\n",
    "# #     print(tok.i, tok, tok.pos_, tok.dep_, tok.head.i, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### TARGETED RETRIEVAL: ATTACKING PEMISES ###\n",
    "\n",
    "# from BERT_adu_classifier import predict\n",
    "\n",
    "# premises = []\n",
    "# for sent in sentences:\n",
    "#     prediction = predict(sent)\n",
    "    \n",
    "#     if prediction == \"premise\":\n",
    "#         premises.append(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b747fec5972a5a28202124dfae2950631b4721a6e18efe99aaae23c73408484"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}