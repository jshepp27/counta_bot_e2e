{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### LOAD ###\n",
    "\n",
    "#keyphrases = [json.loads(ln)[\"keyphrase\"] for ln in open(\"keyphrases.jsonl\")]\n",
    "data = [json.loads(ln) for ln in open(\"../../data/train_cmv_cleaned.jsonl\")]\n",
    "#topics = [json.loads(ln) for ln in open(\"topic_signatures.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../utils/\")\n",
    "\n",
    "import keyphrase_extraction\n",
    "#import utils.keyphrase_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arg': These people are definitely victims but in my opinion referring to them as survivors is in many cases overly dramatic. They might be referred to as survivors if their lives legitimately came into jeopardy if they were held at gunpoint if they were severely physically assaulted etc. but most instances of rape unwanted sexual advances isolated to a single instance do not come attached with a serious possibility of death. Rape is definitely one of the worst things that can happen to somebody but I think that survivor suggests that what happened to them was on the same level as living through a genocidal campaign a terminal illness a plane crash or something else of the sort. The former is bad but the latter are almost certainly worse. Rape victims calling themselves survivors takes attention away from more serious issues in which death is on the line. Again rape is a very serious issue but not as serious as issues that consistently result in death.,\n",
       " 'stance': 'PRO',\n",
       " 'aspect': rape victims}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://aclanthology.org/E17-1024.pdf\n",
    "# TODOs: Polarity Shifters\n",
    "# TODOs: Sentence Levels over the entire argument\n",
    "# MANUAL: Added \"least\"\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from keyphrase_extraction import exctract_keyphrase\n",
    "import random\n",
    "\n",
    "#id = random.randint(0, 2000)\n",
    "# keyphrase = nlp(keyphrases[id][0].lower())\n",
    "#topic = nlp(topics[id][\"topic_signature\"][0])\n",
    "#arg = nlp(data[id][\"argument\"])\n",
    "\n",
    "# TODOs: Sentence Level \n",
    "# TODOs: Argument Level\n",
    "# TODOs: Manage Adjectival Complement 'adcomp'\n",
    "# TODOs: Sentiment Shifters - reverse polarity of Sentiment Words; k trailing words from Sentiment Shifters\n",
    "\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# id = 1586\n",
    "\n",
    "id = random.randint(0, 1000)\n",
    "claim =  nlp(data[id][\"titles\"])\n",
    "arg = nlp(data[id][\"arguments\"])\n",
    "\n",
    "# Aspect\n",
    "keyphrase = exctract_keyphrase(str(arg), n_gram=3)[0]\n",
    "keyphrase = nlp(keyphrase)\n",
    "\n",
    "### SENTIMENT LEXICONS ###\n",
    "pos = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/positive_lex.txt\")]\n",
    "neg = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/negative_lex.txt\")]\n",
    "\n",
    "compound_word = \"\"\n",
    "for i in keyphrase:\n",
    "    if i.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "        comps = \"\".join([str(j) for j in i.children if j.dep_ == \"compound\"])\n",
    "        if comps:\n",
    "            compound_word = comps + \" \" + str(i)\n",
    "\n",
    "# match pattern\n",
    "patterns = [nlp(compound_word)]\n",
    "phrase_matcher.add(\"phrases\", None, *patterns)\n",
    "\n",
    "start = 0\n",
    "stop = 0\n",
    "matched_phrases = phrase_matcher(claim)\n",
    "for i, j, k in matched_phrases:\n",
    "    start = j\n",
    "    stop = k\n",
    "\n",
    "pos_score = 0.0\n",
    "neg_score = 0.0\n",
    "\n",
    "for idx, tok in enumerate(arg):\n",
    "\n",
    "    if idx == start or idx == stop - 1:\n",
    "        continue\n",
    "\n",
    "    # Polarity Shift\n",
    "    # NEAR parameter, k\n",
    "    k = 5\n",
    "    if tok.dep_ == \"neg\":\n",
    "        if tok.text in pos:\n",
    "            # Shift to Negative\n",
    "            if idx <= k:\n",
    "                if idx < start: neg_score += 1/(start - idx)\n",
    "                else: neg_score += 1/(idx - stop)**0.5\n",
    "    \n",
    "        if str(tok.head.text) in neg:\n",
    "            # Shift to Positive\n",
    "            if idx < start: pos_score += 1/(start - idx)\n",
    "            else: pos_score += 1/(idx - stop)**0.5\n",
    "\n",
    "    if str(tok.text) in pos:\n",
    "        if idx < start: pos_score += 1/(start - idx)\n",
    "        else: pos_score += 1/(idx - stop)**0.5\n",
    "\n",
    "    if str(tok.text) in neg:\n",
    "        if idx < start: neg_score += 1/(start - idx)\n",
    "        else: neg_score += 1/(idx - stop)**0.5\n",
    "\n",
    "result = pos_score - neg_score /(pos_score - neg_score + 1)\n",
    "stance = \"\"\n",
    "\n",
    "neg_score, pos_score\n",
    "stance = {\"arg\": arg, \"stance\": \"PRO\", \"aspect\": keyphrase} if result > 0 else {\"claim\": claim, \"stance\": \"CON\", \"aspect\": keyphrase}\n",
    "\n",
    "stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma},\n",
       "  {'claim': I believe society causes more trauma for rape victims,\n",
       "   'stance': 'CON',\n",
       "   'aspect': society causes trauma}],\n",
       " -14)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SENTIMENT LEXICONS ###\n",
    "pos = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/positive_lex.txt\")]\n",
    "neg = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/negative_lex.txt\")]\n",
    "\n",
    "def sentence_stance(sentence):\n",
    "    keyphrase = exctract_keyphrase(str(claim), n_gram=3)[0]\n",
    "    keyphrase = nlp(keyphrase)\n",
    "\n",
    "    compound_word = \"\"\n",
    "    for i in keyphrase:\n",
    "        if i.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            comps = \"\".join([str(j) for j in i.children if j.dep_ == \"compound\"])\n",
    "            if comps:\n",
    "                compound_word = comps + \" \" + str(i)\n",
    "\n",
    "    pos_score = 0.0\n",
    "    neg_score = 0.0\n",
    "\n",
    "    for idx, tok in enumerate(claim):\n",
    "\n",
    "        if idx == start or idx == stop - 1:\n",
    "            continue\n",
    "\n",
    "        # Polarity Shift\n",
    "        # NEAR parameter, k\n",
    "        k = 5\n",
    "        if tok.dep_ == \"neg\":\n",
    "            if tok.text in pos:\n",
    "                # Shift to Negative\n",
    "                if idx <= k:\n",
    "                    if idx < start: neg_score += 1/(start - idx)\n",
    "                    else: neg_score += 1/(idx - stop)**0.5\n",
    "        \n",
    "            if str(tok.head.text) in neg:\n",
    "                # Shift to Positive\n",
    "                if idx < start: pos_score += 1/(start - idx)\n",
    "                else: pos_score += 1/(idx - stop)**0.5\n",
    "\n",
    "        if str(tok.text) in pos:\n",
    "            if idx < start: pos_score += 1/(start - idx)\n",
    "            else: pos_score += 1/(idx - stop)**0.5\n",
    "\n",
    "        if str(tok.text) in neg:\n",
    "            if idx < start: neg_score += 1/(start - idx)\n",
    "            else: neg_score += 1/(idx - stop)**0.5\n",
    "\n",
    "    result = pos_score - neg_score /(pos_score - neg_score + 1)\n",
    "    stance = \"\"\n",
    "\n",
    "    neg_score, pos_score\n",
    "    stance = {\"claim\": claim, \"stance\": \"PRO\", \"aspect\": keyphrase} if result > 0 else {\"claim\": claim, \"stance\": \"CON\", \"aspect\": keyphrase}\n",
    "\n",
    "    return stance\n",
    "\n",
    "id = random.randint(0, 1000)\n",
    "claim =  nlp(data[id][\"titles\"])\n",
    "\n",
    "sentence_stance(claim)\n",
    "arg = nlp(data[id][\"arguments\"])\n",
    "\n",
    "stances = []\n",
    "for i in arg.sents:\n",
    "    stances.append(sentence_stance(str(i)))\n",
    "\n",
    "score = 0\n",
    "for i in stances:\n",
    "    if i[\"stance\"] == \"PRO\":\n",
    "        score += 1\n",
    "    \n",
    "    elif i[\"stance\"] == \"CON\":\n",
    "        score -= 1\n",
    "\n",
    "# Add a main Aspect and Topic Here\n",
    "stances, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertopic import BERTopic\n",
    "# import scipy\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# topic_model = BERTopic()\n",
    "# #sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# # topic_model = BERTopic(embedding_model=sentence_model)\n",
    "\n",
    "# arg = [i[\"arguments\"] for i in data]\n",
    "\n",
    "# topics, probs = topic_model.fit_transform(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pardata\n",
    "data = pardata.load_dataset('wikipedia_category_stance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full':      Label (-/P/C/?)    Concept  \\\n",
       " 0                   P  Abortion   \n",
       " 1                   P  Abortion   \n",
       " 2                   C  Abortion   \n",
       " 3                   -  Abortion   \n",
       " 4                   P  Abortion   \n",
       " ...               ...       ...   \n",
       " 4598                -   Zionism   \n",
       " 4599                -   Zionism   \n",
       " 4600                P   Zionism   \n",
       " 4601                P   Zionism   \n",
       " 4602                P   Zionism   \n",
       " \n",
       "                                Category/List Page Title  \\\n",
       " 0                  Category:American_abortion_providers   \n",
       " 1                Category:American_pro-choice_activists   \n",
       " 2                  Category:American_pro-life_activists   \n",
       " 3     Category:Anti-abortion_violence_in_the_United_...   \n",
       " 4                 Category:British_pro-choice_activists   \n",
       " ...                                                 ...   \n",
       " 4598                                   Category:Zionism   \n",
       " 4599              Category:Zionism_in_the_United_States   \n",
       " 4600  Category:Zionist_Socialist_Workers_Party_polit...   \n",
       " 4601                                  Category:Zionists   \n",
       " 4602                                   List_of_Zionists   \n",
       " \n",
       "                                                     URL  \n",
       " 0     https://en.wikipedia.org/wiki/Category:America...  \n",
       " 1     https://en.wikipedia.org/wiki/Category:America...  \n",
       " 2     https://en.wikipedia.org/wiki/Category:America...  \n",
       " 3     https://en.wikipedia.org/wiki/Category:Anti-ab...  \n",
       " 4     https://en.wikipedia.org/wiki/Category:British...  \n",
       " ...                                                 ...  \n",
       " 4598     https://en.wikipedia.org/wiki/Category:Zionism  \n",
       " 4599  https://en.wikipedia.org/wiki/Category:Zionism...  \n",
       " 4600  https://en.wikipedia.org/wiki/Category:Zionist...  \n",
       " 4601    https://en.wikipedia.org/wiki/Category:Zionists  \n",
       " 4602     https://en.wikipedia.org/wiki/List_of_Zionists  \n",
       " \n",
       " [4603 rows x 4 columns]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='dax.cdn.appdomain.cloud', port=443): Max retries exceeded with url: /dax-wikipedia-category-stance/1.0.2/wikipedia-category-stance.tar.gz (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x2af711630>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/urllib3/util/connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m six\u001b[39m.\u001b[39mraise_from(\n\u001b[1;32m     69\u001b[0m         LocationParseError(\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m host), \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[1;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/urllib3/connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x2af711630>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='dax.cdn.appdomain.cloud', port=443): Max retries exceeded with url: /dax-wikipedia-category-stance/1.0.2/wikipedia-category-stance.tar.gz (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x2af711630>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/detection/stance_classification_.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/detection/stance_classification_.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m fname \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mwikipedia-category-stance.tar.gz\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/detection/stance_classification_.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(url_base, version, fname)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/detection/stance_classification_.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/detection/stance_classification_.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m r\u001b[39m.\u001b[39mok:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/detection/stance_classification_.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThere are some errors when downloading \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(url))\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/requests/adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    567\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='dax.cdn.appdomain.cloud', port=443): Max retries exceeded with url: /dax-wikipedia-category-stance/1.0.2/wikipedia-category-stance.tar.gz (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x2af711630>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))"
     ]
    }
   ],
   "source": [
    "# import requests # External dependency: pip install requests\n",
    "# import tarfile\n",
    "\n",
    "# # Downloading the dataset\n",
    "# url_base = 'https://dax.cdn.appdomain.cloud/dax-wikipedia-category-stance'\n",
    "# version = '1.0.2'\n",
    "# fname = 'wikipedia-category-stance.tar.gz'\n",
    "# url = \"{}/{}/{}\".format(url_base, version, fname)\n",
    "# r = requests.get(url)\n",
    "\n",
    "# if not r.ok:\n",
    "#     print(\"There are some errors when downloading {}\".format(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "American patriots have a general mentality against immigration. Saying these people shouldnt be allowed to become part of the country is the same as saying they are worse because they were born on a different side of a line and americans are better because of the side of the line they are born on. This is prominent in many ads and political champagnes namely the slogan Creating jobs for americans. I understand why politics use this slogan because they are trying to get americans to vote for them but this slogan is also prominent in ads made by private corporations. As if creating jobs for americans is morally superior to creating jobs for people of other countries. The companies launching these ads may be trying to win in the american market so they can sell more of their product but the fact that this can increase sales shows that many americans hold the being born on one side of a line belief. I am not blaming the politicians or corporations running ads running these slogans they are merely trying to win votes or make money from this mentality but really it is the citizens that have the belief that they are better because of the side of a line they are born on that are at fault. Patriotism is really just this belief."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = random.randint(0, 1000)\n",
    "arg = nlp(data[id][\"arguments\"])\n",
    "arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02336907386779785,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 1625569391,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfb9a5076eb4b4a96a81bdd11a911a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/detection/stance_classification_.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/detection/stance_classification_.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcristian-popa/bart-tl-ng\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/detection/stance_classification_.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(mname)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/detection/stance_classification_.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39;49mfrom_pretrained(mname)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/detection/stance_classification_.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msite web google search website online internet social content user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/counta_bot_e2e/counta_bot/detection/stance_classification_.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m enc \u001b[39m=\u001b[39m tokenizer(\u001b[39minput\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:446\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    445\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    447\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m )\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1940\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m     archive_file \u001b[39m=\u001b[39m hf_bucket_url(\n\u001b[1;32m   1931\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   1932\u001b[0m         filename\u001b[39m=\u001b[39mfilename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1935\u001b[0m         subfolder\u001b[39m=\u001b[39msubfolder \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(subfolder) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1936\u001b[0m     )\n\u001b[1;32m   1938\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1939\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m-> 1940\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_path(\n\u001b[1;32m   1941\u001b[0m         archive_file,\n\u001b[1;32m   1942\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1943\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1944\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1945\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1946\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1947\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1948\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1949\u001b[0m     )\n\u001b[1;32m   1951\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m   1952\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1953\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1954\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1955\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtoken having permission to this repo with `use_auth_token` or log in with `huggingface-cli \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1956\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlogin` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1957\u001b[0m     )\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/transformers/utils/hub.py:284\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    280\u001b[0m     local_files_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    283\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    285\u001b[0m         url_or_filename,\n\u001b[1;32m    286\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    287\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    288\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    289\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    290\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    291\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    292\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    293\u001b[0m     )\n\u001b[1;32m    294\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    295\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/transformers/utils/hub.py:604\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[39m# The url_to_download might be messy, so we extract the file name from the original url.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m     file_name \u001b[39m=\u001b[39m url\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 604\u001b[0m     http_get(\n\u001b[1;32m    605\u001b[0m         url_to_download,\n\u001b[1;32m    606\u001b[0m         temp_file,\n\u001b[1;32m    607\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    608\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m    609\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    610\u001b[0m         file_name\u001b[39m=\u001b[39;49mfile_name,\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    613\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mcache_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    614\u001b[0m os\u001b[39m.\u001b[39mreplace(temp_file\u001b[39m.\u001b[39mname, cache_path)\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/transformers/utils/hub.py:453\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, file_name)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39m# `tqdm` behavior is determined by `utils.logging.is_progress_bar_enabled()`\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[39m# and can be set using `utils.logging.enable/disable_progress_bar()`\u001b[39;00m\n\u001b[1;32m    445\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    446\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    447\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m     desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading \u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m file_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mDownloading\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    452\u001b[0m )\n\u001b[0;32m--> 453\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m    454\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    455\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/urllib3/response.py:627\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 627\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    629\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    630\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/urllib3/response.py:566\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    563\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    565\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 566\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_e2e/venv/lib/python3.10/site-packages/urllib3/response.py:532\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    530\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.6_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "mname = \"cristian-popa/bart-tl-ng\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(mname)\n",
    "\n",
    "input = \"site web google search website online internet social content user\"\n",
    "enc = tokenizer(input, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "outputs = model.generate(\n",
    "    input_ids=enc.input_ids,\n",
    "    attention_mask=enc.attention_mask,\n",
    "    max_length=15,\n",
    "    min_length=1,\n",
    "    do_sample=False,\n",
    "    num_beams=25,\n",
    "    length_penalty=1.0,\n",
    "    repetition_penalty=1.5\n",
    ")\n",
    "\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.30126753, 0.        , ..., 0.        , 1.        ,\n",
       "       0.44818607])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TOPICS ###\n",
    "# https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "# embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### CONCEPT NET ###\n",
    "# import requests\n",
    "\n",
    "# obj = requests.get(f\"http://api.conceptnet.io/c/en/{stance['aspect']}\").json()\n",
    "# obj.keys()\n",
    "\n",
    "# for i in obj[\"edges\"]:\n",
    "#     if i[\"start\"][\"language\"] == \"en\":\n",
    "#         print(\"start\", i[\"start\"])\n",
    "#         print(\"rel\", i[\"rel\"])\n",
    "#         print(\"rel\", i[\"rel\"])\n",
    "#         print(i[\"start\"][\"label\"])\n",
    "#         print(\"rel\", i[\"rel\"][\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// The Round Table\n",
      "digraph {\n",
      "\tA [label=\"King Arthur\"]\n",
      "\tB [label=\"Sir Bedevere the Wise\"]\n",
      "\tL [label=\"Sir Lancelot the Brave\"]\n",
      "\tA -> B\n",
      "\tA -> L\n",
      "\tB -> L [constraint=false]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'doctest-output/round-table.gv.pdf'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import graphviz\n",
    "# from graphviz import Source\n",
    "# from nltk import Tree\n",
    "# import deplacy\n",
    "\n",
    "# # sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "# # doc = nlp(sentence)\n",
    "\n",
    "# dot = graphviz.Digraph(comment='The Round Table')\n",
    "# dot.node('A', 'King Arthur')  # doctest: +NO_EXE\n",
    "# dot.node('B', 'Sir Bedevere the Wise')\n",
    "# dot.node('L', 'Sir Lancelot the Brave')\n",
    "\n",
    "# dot.edges(['AB', 'AL'])\n",
    "# dot.edge('B', 'L', constraint='false')\n",
    "\n",
    "# print(dot.source)\n",
    "# dot.render('doctest-output/round-table.gv').replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumps ROOT VERB [fox, over, .]\n",
      "fox nsubj NOUN [The, quick, brown]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "edge() missing 1 required positional argument: 'head_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [255]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         dot\u001b[38;5;241m.\u001b[39mnode(node\u001b[38;5;241m.\u001b[39morth_)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mto_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [255]\u001b[0m, in \u001b[0;36mto_tree\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mn_lefts \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39mn_rights \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(node\u001b[38;5;241m.\u001b[39morth_, node\u001b[38;5;241m.\u001b[39mdep_, node\u001b[38;5;241m.\u001b[39mpos_, [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren])\n\u001b[0;32m----> 7\u001b[0m     dot\u001b[38;5;241m.\u001b[39medge([to_tree(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren])\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# [to_tree(i) for i in node.children]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     dot\u001b[38;5;241m.\u001b[39mnode(node\u001b[38;5;241m.\u001b[39morth_)\n",
      "Input \u001b[0;32mIn [255]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mn_lefts \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39mn_rights \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(node\u001b[38;5;241m.\u001b[39morth_, node\u001b[38;5;241m.\u001b[39mdep_, node\u001b[38;5;241m.\u001b[39mpos_, [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren])\n\u001b[0;32m----> 7\u001b[0m     dot\u001b[38;5;241m.\u001b[39medge([\u001b[43mto_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren])\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# [to_tree(i) for i in node.children]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     dot\u001b[38;5;241m.\u001b[39mnode(node\u001b[38;5;241m.\u001b[39morth_)\n",
      "Input \u001b[0;32mIn [255]\u001b[0m, in \u001b[0;36mto_tree\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mn_lefts \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39mn_rights \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(node\u001b[38;5;241m.\u001b[39morth_, node\u001b[38;5;241m.\u001b[39mdep_, node\u001b[38;5;241m.\u001b[39mpos_, [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren])\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mdot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mto_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# [to_tree(i) for i in node.children]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     dot\u001b[38;5;241m.\u001b[39mnode(node\u001b[38;5;241m.\u001b[39morth_)\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_/venv/lib/python3.9/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: edge() missing 1 required positional argument: 'head_name'"
     ]
    }
   ],
   "source": [
    "# doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "\n",
    "# dot = graphviz.Digraph(comment='Test')\n",
    "# def to_tree(node):\n",
    "#     if node.n_lefts + node.n_rights > 0:\n",
    "#         print(node.orth_, node.dep_, node.pos_, [i for i in node.children])\n",
    "#         # dot.edge([to_tree(i) for i in node.children])\n",
    "#         # [to_tree(i) for i in node.children]\n",
    "#     else:node.orth_,\n",
    "#         dot.node(node.orth_)\n",
    "\n",
    "# for sent in doc.sents:\n",
    "#     to_tree(sent.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                questions                    \n",
      "  __________________|________________         \n",
      " |   |   |          |              teach     \n",
      " |   |   |          |           _____|____    \n",
      " |   |   |        answer       |     |   most\n",
      " |   |   |    ______|______    |     |    |   \n",
      "its the  .   we     ca    n't that   us  the \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import spacy\n",
    "# from nltk import Tree\n",
    "\n",
    "# doc = nlp(\"its the questions we can't answer that teach us the most.\")\n",
    "\n",
    "# def to_nltk_tree(node):\n",
    "#     if node.n_lefts + node.n_rights > 0:\n",
    "#         return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "#     else:\n",
    "#         return node.orth_\n",
    "\n",
    "\n",
    "# [to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insurance liability\n"
     ]
    }
   ],
   "source": [
    "# doc = nlp(\"Autonomous cars and magic wands shift insurance liability toward manufacturers\")\n",
    "# interesting_token = doc[7] # or however you identify the token you want\n",
    "# for noun_chunk in doc.noun_chunks:\n",
    "#     if interesting_token in noun_chunk:\n",
    "#         print(noun_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans are going to die out because of an economic collapse\n",
      "an economic collapse []\n"
     ]
    }
   ],
   "source": [
    "# for chunk in sent_.noun_chunks:\n",
    "#     #print(chunk.text)\n",
    "#     if chunk.text in str(pattern):\n",
    "#         print(chunk.text, [i for i in chunk.lefts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### SENTIMENT LEXICONS ###\n",
    "# pos = [w.replace(\"\\n\", \"\") for w in open(\"positive_lex.txt\")]\n",
    "# neg = [w.replace(\"\\n\", \"\") for w in open(\"negative_lex.txt\")]\n",
    "\n",
    "# id = 2\n",
    "# claim =  data[id][\"titles\"] + data[id][\"arguments\"]\n",
    "# doc = nlp(claim.lower())\n",
    "#\n",
    "# pattern = nlp(keyphrases[id][0].lower())\n",
    "#\n",
    "# compound_word = \"\"\n",
    "# for i in pattern:\n",
    "#     if i.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "#         comps = \"\".join([str(j) for j in i.children if j.dep_ == \"compound\"])\n",
    "#         if comps:\n",
    "#             compound_word = comps + \" \" + str(i)\n",
    "#\n",
    "#\n",
    "#\n",
    "# opinion_sents = []\n",
    "#\n",
    "# for i in doc.sents:\n",
    "#     if str(compound_word) in str(i):\n",
    "#         opinion_sents.append(i)\n",
    "#\n",
    "# stance_distances = {}\n",
    "# for i, sent in enumerate(opinion_sents):\n",
    "#     stance_distances[i] = {k: [] for k in sent}\n",
    "#\n",
    "#\n",
    "# # test = [i for i in opinion_sents[0]]\n",
    "# # for tok in test:\n",
    "# #     if str(tok.text) in list(str(stance_distances[i].keys())):\n",
    "# #         print(tok.text)\n",
    "# #token.text in list(stance_distances[i].keys()):\n",
    "#\n",
    "# for i, sent in enumerate(opinion_sents):\n",
    "#\n",
    "#     for count, token in enumerate(sent):\n",
    "#         print(count, token)\n",
    "#\n",
    "#         if str(token.text) in list(str(l) for l in stance_distances[i].keys()):\n",
    "#             pass\n",
    "#             # if str(token.text) in pos:\n",
    "#             #     stance_distances[i][token.text].append((\"POS\", count))\n",
    "# #\n",
    "#             # if token.text in neg:\n",
    "#             #     # stance_distances[i][token.text] = (\"NEG\", count)\n",
    "#             #     stance_distances[i][token.text].append((\"NEG\", count))\n",
    "#\n",
    "# #             # else: stance_distances[i][token.text] = (\"UNK\", count)\n",
    "# #\n",
    "#\n",
    "# print(stance_distances)\n",
    "#\n",
    "# # print(\"benefits\" in pos)\n",
    "# # print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# id = 2\n",
    "# kp = str(\" \".join([i for i in keyphrases[id]]))\n",
    "# kp_doc = nlp(kp)\n",
    "#\n",
    "# kp_nouns = []\n",
    "# kp_nouns.append(\" \".join([str(tok) for tok in kp_doc if tok.pos_ in ('NOUN','PROPN')]))\n",
    "# kp_nouns.append(\" \".join(([str(tok) for tok in kp_doc if tok.dep_ == 'compound'])))\n",
    "#\n",
    "# kp_nouns = \" \".join(kp_nouns)\n",
    "# print(kp_nouns)\n",
    "#\n",
    "# pos = [w for w in open(\"positive_lex.txt\")]\n",
    "# neg = [w for w in open(\"negative_lex.txt\")]\n",
    "#\n",
    "# adj_ = []\n",
    "# stance = 0\n",
    "#\n",
    "# for i in noun_adj_pairs:\n",
    "#\n",
    "#     j, k = i\n",
    "#\n",
    "#     if j.text in kp_nouns:\n",
    "#\n",
    "#         if j.text in pos: stance = 1\n",
    "#         if j.text in neg: stance = -1\n",
    "#\n",
    "#         else: stance = \"UNK\"\n",
    "#\n",
    "#         adj_.append((k, stance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# id = 50\n",
    "# sent_ = data[id][\"titles\"] + data[id][\"arguments\"]\n",
    "# doc = nlp(sent_)\n",
    "#\n",
    "# # Consider Childeren == compound etc\n",
    "# noun_adj_pairs = []\n",
    "# for i,token in enumerate(doc):\n",
    "#\n",
    "#     if token.pos_ not in ('NOUN','PROPN'):\n",
    "#         continue\n",
    "#\n",
    "#     for j in range(i+1,len(doc)):\n",
    "#         if doc[j].pos_ == 'ADJ' or doc[j].pos_ == \"CCONJ\":\n",
    "#             noun_adj_pairs.append((token,doc[j]))\n",
    "#             break\n",
    "#\n",
    "# noun_adj_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parsed_sentence = nlp(u'This is my sentence')\n",
    "# [(token.text,token.i) for token in parsed_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pos = [w.replace(\"\\n\", \"\") for w in open(\"positive_lex.txt\")]\n",
    "# neg = [w.replace(\"\\n\", \"\") for w in open(\"negative_lex.txt\")]\n",
    "#\n",
    "# id = 2\n",
    "# claim =  data[id][\"titles\"] + data[id][\"arguments\"]\n",
    "# doc = nlp(claim.lower())\n",
    "#\n",
    "# pattern = nlp(keyphrases[id][0].lower())\n",
    "# print(pattern)\n",
    "#\n",
    "# compound_word = \"\"\n",
    "# for i in pattern:\n",
    "#     if i.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "#         comps = \"\".join([str(j) for j in i.children if j.dep_ == \"compound\"])\n",
    "#         if comps:\n",
    "#             compound_word = comps + \" \" + str(i)\n",
    "#\n",
    "# compound_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the targets of the given topic, claim\n",
    "# Identify the polarity (sentiment) towards each of the targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from spacy.symbols import nsubj, VERB\n",
    "#\n",
    "# doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "#\n",
    "# verbs = set()\n",
    "# for possible_subject in doc:\n",
    "#     if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "#         verbs.add(possible_subject.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# OPEN-IE\n",
    "\n",
    "# IMPORTANT NOTE; Command: java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "# TODOs: Use openie library, StanfordOpenIE\n",
    "\n",
    "# import nltk\n",
    "# from pycorenlp import *\n",
    "# import json\n",
    "# import collections\n",
    "#\n",
    "# nlp = StanfordCoreNLP(\"http://localhost:9000/\")\n",
    "# properties = {\n",
    "#     \"annotators\": \"tokenize, ssplit, pos, depparse, natlog,openie\",\n",
    "#     \"outputFormat\": \"json\",\n",
    "# }\n",
    "#\n",
    "# output = []\n",
    "# sub_pred_obj = []\n",
    "# titles = []\n",
    "\n",
    "# for j in data[13:14]:\n",
    "#     result = json.loads(nlp.annotate(j[\"titles\"], properties=properties))\n",
    "#     raw = result[\"sentences\"][0][\"openie\"]\n",
    "#     if raw:\n",
    "#         output.append([j[\"titles\"], result[\"sentences\"][0][\"openie\"]])\n",
    "#     else:\n",
    "#         continue\n",
    "#\n",
    "# # for i in output:\n",
    "# #     sub_pred_obj.append({\n",
    "# #         \"title\": i[1],\n",
    "# #         \"subject\": i[3][0][\"subject\"],\n",
    "# #         \"predicate\": i[3][0][\"relation\"],\n",
    "# #         \"object\": i[3][0][\"object\"]\n",
    "# #     })\n",
    "#\n",
    "# for i in sub_pred_obj:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'I believe that the Gay Marriage discussion isnt as important': [Gay Marriage, Marriage discussion]}]\n"
     ]
    }
   ],
   "source": [
    "# Nice Implementation\n",
    "\n",
    "# texts =  (\"I believe that the Gay Marriage discussion isnt as important\",\n",
    "#          )\n",
    "# docs = nlp.pipe(texts)\n",
    "#\n",
    "# compounds = []\n",
    "# for doc in docs:\n",
    "#     compounds.append({doc.text:[doc[tok.i:tok.head.i+1] for tok in doc if tok.dep_==\"compound\"]})\n",
    "# print(compounds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b747fec5972a5a28202124dfae2950631b4721a6e18efe99aaae23c73408484"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
