{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### LOAD ###\n",
    "\n",
    "#keyphrases = [json.loads(ln)[\"keyphrase\"] for ln in open(\"keyphrases.jsonl\")]\n",
    "data = [json.loads(ln) for ln in open(\"../data/train_cmv_cleaned.jsonl\")]\n",
    "#topics = [json.loads(ln) for ln in open(\"topic_signatures.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import keyphrase_extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keyphrase_extraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/joshua.sheppard/PycharmProjects/retriever/counta_bot/detection/stance_classification_.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/counta_bot/detection/stance_classification_.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# https://aclanthology.org/E17-1024.pdf\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/counta_bot/detection/stance_classification_.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# TODOs: Polarity Shifters\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/counta_bot/detection/stance_classification_.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# TODOs: Sentence Levels over the entire argument\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/counta_bot/detection/stance_classification_.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# MANUAL: Added \"least\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/counta_bot/detection/stance_classification_.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeyphrase_extraction\u001b[39;00m \u001b[39mimport\u001b[39;00m exctract_keyphrase\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/counta_bot/detection/stance_classification_.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatcher\u001b[39;00m \u001b[39mimport\u001b[39;00m PhraseMatcher\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joshua.sheppard/PycharmProjects/retriever/counta_bot/detection/stance_classification_.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keyphrase_extraction'"
     ]
    }
   ],
   "source": [
    "# https://aclanthology.org/E17-1024.pdf\n",
    "# TODOs: Polarity Shifters\n",
    "# TODOs: Sentence Levels over the entire argument\n",
    "# MANUAL: Added \"least\"\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import random\n",
    "\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "#id = random.randint(0, 2000)\n",
    "id = 1586\n",
    "# keyphrase = nlp(keyphrases[id][0].lower())\n",
    "#topic = nlp(topics[id][\"topic_signature\"][0])\n",
    "\n",
    "claim =  nlp(data[id][\"titles\"])\n",
    "arg = nlp(data[id][\"argument\"])\n",
    "keyphrase = exctract_keyphrase(claim)\n",
    "\n",
    "\"scripts/lexicon/negative_lex.txt\"\n",
    "# ### SENTIMENT LEXICONS ###\n",
    "pos = [w.replace(\"\\n\", \"\") for w in open(\"./lexicon/positive_lex.txt\")]\n",
    "neg = [w.replace(\"\\n\", \"\") for w in open(\"./lexicon/negative_lex.txt\")]\n",
    "\n",
    "compound_word = \"\"\n",
    "for i in keyphrase:\n",
    "    if i.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "        comps = \"\".join([str(j) for j in i.children if j.dep_ == \"compound\"])\n",
    "        if comps:\n",
    "            compound_word = comps + \" \" + str(i)\n",
    "\n",
    "# match pattern\n",
    "patterns = [nlp(compound_word)]\n",
    "phrase_matcher.add(\"phrases\", None, *patterns)\n",
    "\n",
    "start = 0\n",
    "stop = 0\n",
    "matched_phrases = phrase_matcher(claim)\n",
    "for i, j, k in matched_phrases:\n",
    "    start = j\n",
    "    stop = k\n",
    "\n",
    "pos_score = 0.0\n",
    "neg_score = 0.0\n",
    "\n",
    "# TODOs - Manage Adjectival Complement 'adcomp'\n",
    "# Sentiment Shifters - reverse polarity of Sentiment Words; k trailing words from Sentiment Shifters\n",
    "\n",
    "for idx, tok in enumerate(claim):\n",
    "\n",
    "    if idx == start or idx == stop - 1:\n",
    "        continue\n",
    "\n",
    "    # Polarity Shift\n",
    "    # NEAR parameter, k\n",
    "    # k = 5\n",
    "    # if tok.dep_ == \"neg\":\n",
    "    #     if tok.text in pos:\n",
    "    #         # Shift to Negative\n",
    "    #         if idx <= k:\n",
    "    #             if idx < start: neg_score += 1/(start - idx)\n",
    "    #             else: neg_score += 1/(idx - stop)**0.5\n",
    "    #\n",
    "    #     if str(tok.head.text) in neg:\n",
    "    #         # Shift to Positive\n",
    "    #         if idx < start: pos_score += 1/(start - idx)\n",
    "    #         else: pos_score += 1/(idx - stop)**0.5\n",
    "\n",
    "    if str(tok.text) in pos:\n",
    "        if idx < start: pos_score += 1/(start - idx)\n",
    "        else: pos_score += 1/(idx - stop)**0.5\n",
    "\n",
    "    if str(tok.text) in neg:\n",
    "        if idx < start: neg_score += 1/(start - idx)\n",
    "        else: neg_score += 1/(idx - stop)**0.5\n",
    "\n",
    "result = pos_score - neg_score /(pos_score - neg_score + 1)\n",
    "stance = \"\"\n",
    "\n",
    "neg_score, pos_score\n",
    "stance = {\"claim\": claim, \"stance\": \"PRO\", \"aspect\": keyphrase, \"topic\": topic} if result > 0 else {\"claim\": claim, \"stance\": \"CON\", \"aspect\": keyphrase, \"topic\": topic}\n",
    "\n",
    "stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start {'@id': '/c/en/retail/v/wn/commerce', '@type': 'Node', 'label': 'retail', 'language': 'en', 'sense_label': 'v, commerce', 'term': '/c/en/retail'}\n",
      "rel {'@id': '/r/MannerOf', '@type': 'Relation', 'label': 'MannerOf'}\n",
      "rel {'@id': '/r/MannerOf', '@type': 'Relation', 'label': 'MannerOf'}\n",
      "retail\n",
      "rel MannerOf\n",
      "start {'@id': '/c/en/retail/n/wn/act', '@type': 'Node', 'label': 'retail', 'language': 'en', 'sense_label': 'n, act', 'term': '/c/en/retail'}\n",
      "rel {'@id': '/r/IsA', '@type': 'Relation', 'label': 'IsA'}\n",
      "rel {'@id': '/r/IsA', '@type': 'Relation', 'label': 'IsA'}\n",
      "retail\n",
      "rel IsA\n",
      "start {'@id': '/c/en/retail/v/wn/commerce', '@type': 'Node', 'label': 'retail', 'language': 'en', 'sense_label': 'v, commerce', 'term': '/c/en/retail'}\n",
      "rel {'@id': '/r/HasContext', '@type': 'Relation', 'label': 'HasContext'}\n",
      "rel {'@id': '/r/HasContext', '@type': 'Relation', 'label': 'HasContext'}\n",
      "retail\n",
      "rel HasContext\n"
     ]
    }
   ],
   "source": [
    "### CONCEPT NET ###\n",
    "import requests\n",
    "\n",
    "obj = requests.get(f\"http://api.conceptnet.io/c/en/{stance['aspect']}\").json()\n",
    "obj.keys()\n",
    "\n",
    "for i in obj[\"edges\"]:\n",
    "    if i[\"start\"][\"language\"] == \"en\":\n",
    "        print(\"start\", i[\"start\"])\n",
    "        print(\"rel\", i[\"rel\"])\n",
    "        print(\"rel\", i[\"rel\"])\n",
    "        print(i[\"start\"][\"label\"])\n",
    "        print(\"rel\", i[\"rel\"][\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// The Round Table\n",
      "digraph {\n",
      "\tA [label=\"King Arthur\"]\n",
      "\tB [label=\"Sir Bedevere the Wise\"]\n",
      "\tL [label=\"Sir Lancelot the Brave\"]\n",
      "\tA -> B\n",
      "\tA -> L\n",
      "\tB -> L [constraint=false]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'doctest-output/round-table.gv.pdf'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "from graphviz import Source\n",
    "from nltk import Tree\n",
    "import deplacy\n",
    "\n",
    "# sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "# doc = nlp(sentence)\n",
    "\n",
    "dot = graphviz.Digraph(comment='The Round Table')\n",
    "dot.node('A', 'King Arthur')  # doctest: +NO_EXE\n",
    "dot.node('B', 'Sir Bedevere the Wise')\n",
    "dot.node('L', 'Sir Lancelot the Brave')\n",
    "\n",
    "dot.edges(['AB', 'AL'])\n",
    "dot.edge('B', 'L', constraint='false')\n",
    "\n",
    "print(dot.source)\n",
    "dot.render('doctest-output/round-table.gv').replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumps ROOT VERB [fox, over, .]\n",
      "fox nsubj NOUN [The, quick, brown]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "edge() missing 1 required positional argument: 'head_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [255]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         dot\u001b[38;5;241m.\u001b[39mnode(node\u001b[38;5;241m.\u001b[39morth_)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mto_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [255]\u001b[0m, in \u001b[0;36mto_tree\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mn_lefts \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39mn_rights \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(node\u001b[38;5;241m.\u001b[39morth_, node\u001b[38;5;241m.\u001b[39mdep_, node\u001b[38;5;241m.\u001b[39mpos_, [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren])\n\u001b[0;32m----> 7\u001b[0m     dot\u001b[38;5;241m.\u001b[39medge([to_tree(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren])\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# [to_tree(i) for i in node.children]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     dot\u001b[38;5;241m.\u001b[39mnode(node\u001b[38;5;241m.\u001b[39morth_)\n",
      "Input \u001b[0;32mIn [255]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mn_lefts \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39mn_rights \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(node\u001b[38;5;241m.\u001b[39morth_, node\u001b[38;5;241m.\u001b[39mdep_, node\u001b[38;5;241m.\u001b[39mpos_, [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren])\n\u001b[0;32m----> 7\u001b[0m     dot\u001b[38;5;241m.\u001b[39medge([\u001b[43mto_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren])\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# [to_tree(i) for i in node.children]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     dot\u001b[38;5;241m.\u001b[39mnode(node\u001b[38;5;241m.\u001b[39morth_)\n",
      "Input \u001b[0;32mIn [255]\u001b[0m, in \u001b[0;36mto_tree\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mn_lefts \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39mn_rights \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(node\u001b[38;5;241m.\u001b[39morth_, node\u001b[38;5;241m.\u001b[39mdep_, node\u001b[38;5;241m.\u001b[39mpos_, [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren])\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mdot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mto_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# [to_tree(i) for i in node.children]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     dot\u001b[38;5;241m.\u001b[39mnode(node\u001b[38;5;241m.\u001b[39morth_)\n",
      "File \u001b[0;32m~/PycharmProjects/counta_bot_/venv/lib/python3.9/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: edge() missing 1 required positional argument: 'head_name'"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "\n",
    "dot = graphviz.Digraph(comment='Test')\n",
    "def to_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        print(node.orth_, node.dep_, node.pos_, [i for i in node.children])\n",
    "        # dot.edge([to_tree(i) for i in node.children])\n",
    "        # [to_tree(i) for i in node.children]\n",
    "    else:node.orth_,\n",
    "        dot.node(node.orth_)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    to_tree(sent.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                questions                    \n",
      "  __________________|________________         \n",
      " |   |   |          |              teach     \n",
      " |   |   |          |           _____|____    \n",
      " |   |   |        answer       |     |   most\n",
      " |   |   |    ______|______    |     |    |   \n",
      "its the  .   we     ca    n't that   us  the \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "doc = nlp(\"its the questions we can't answer that teach us the most.\")\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insurance liability\n"
     ]
    }
   ],
   "source": [
    "# doc = nlp(\"Autonomous cars and magic wands shift insurance liability toward manufacturers\")\n",
    "# interesting_token = doc[7] # or however you identify the token you want\n",
    "# for noun_chunk in doc.noun_chunks:\n",
    "#     if interesting_token in noun_chunk:\n",
    "#         print(noun_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans are going to die out because of an economic collapse\n",
      "an economic collapse []\n"
     ]
    }
   ],
   "source": [
    "# for chunk in sent_.noun_chunks:\n",
    "#     #print(chunk.text)\n",
    "#     if chunk.text in str(pattern):\n",
    "#         print(chunk.text, [i for i in chunk.lefts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### SENTIMENT LEXICONS ###\n",
    "# pos = [w.replace(\"\\n\", \"\") for w in open(\"positive_lex.txt\")]\n",
    "# neg = [w.replace(\"\\n\", \"\") for w in open(\"negative_lex.txt\")]\n",
    "\n",
    "# id = 2\n",
    "# claim =  data[id][\"titles\"] + data[id][\"arguments\"]\n",
    "# doc = nlp(claim.lower())\n",
    "#\n",
    "# pattern = nlp(keyphrases[id][0].lower())\n",
    "#\n",
    "# compound_word = \"\"\n",
    "# for i in pattern:\n",
    "#     if i.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "#         comps = \"\".join([str(j) for j in i.children if j.dep_ == \"compound\"])\n",
    "#         if comps:\n",
    "#             compound_word = comps + \" \" + str(i)\n",
    "#\n",
    "#\n",
    "#\n",
    "# opinion_sents = []\n",
    "#\n",
    "# for i in doc.sents:\n",
    "#     if str(compound_word) in str(i):\n",
    "#         opinion_sents.append(i)\n",
    "#\n",
    "# stance_distances = {}\n",
    "# for i, sent in enumerate(opinion_sents):\n",
    "#     stance_distances[i] = {k: [] for k in sent}\n",
    "#\n",
    "#\n",
    "# # test = [i for i in opinion_sents[0]]\n",
    "# # for tok in test:\n",
    "# #     if str(tok.text) in list(str(stance_distances[i].keys())):\n",
    "# #         print(tok.text)\n",
    "# #token.text in list(stance_distances[i].keys()):\n",
    "#\n",
    "# for i, sent in enumerate(opinion_sents):\n",
    "#\n",
    "#     for count, token in enumerate(sent):\n",
    "#         print(count, token)\n",
    "#\n",
    "#         if str(token.text) in list(str(l) for l in stance_distances[i].keys()):\n",
    "#             pass\n",
    "#             # if str(token.text) in pos:\n",
    "#             #     stance_distances[i][token.text].append((\"POS\", count))\n",
    "# #\n",
    "#             # if token.text in neg:\n",
    "#             #     # stance_distances[i][token.text] = (\"NEG\", count)\n",
    "#             #     stance_distances[i][token.text].append((\"NEG\", count))\n",
    "#\n",
    "# #             # else: stance_distances[i][token.text] = (\"UNK\", count)\n",
    "# #\n",
    "#\n",
    "# print(stance_distances)\n",
    "#\n",
    "# # print(\"benefits\" in pos)\n",
    "# # print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# id = 2\n",
    "# kp = str(\" \".join([i for i in keyphrases[id]]))\n",
    "# kp_doc = nlp(kp)\n",
    "#\n",
    "# kp_nouns = []\n",
    "# kp_nouns.append(\" \".join([str(tok) for tok in kp_doc if tok.pos_ in ('NOUN','PROPN')]))\n",
    "# kp_nouns.append(\" \".join(([str(tok) for tok in kp_doc if tok.dep_ == 'compound'])))\n",
    "#\n",
    "# kp_nouns = \" \".join(kp_nouns)\n",
    "# print(kp_nouns)\n",
    "#\n",
    "# pos = [w for w in open(\"positive_lex.txt\")]\n",
    "# neg = [w for w in open(\"negative_lex.txt\")]\n",
    "#\n",
    "# adj_ = []\n",
    "# stance = 0\n",
    "#\n",
    "# for i in noun_adj_pairs:\n",
    "#\n",
    "#     j, k = i\n",
    "#\n",
    "#     if j.text in kp_nouns:\n",
    "#\n",
    "#         if j.text in pos: stance = 1\n",
    "#         if j.text in neg: stance = -1\n",
    "#\n",
    "#         else: stance = \"UNK\"\n",
    "#\n",
    "#         adj_.append((k, stance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# id = 50\n",
    "# sent_ = data[id][\"titles\"] + data[id][\"arguments\"]\n",
    "# doc = nlp(sent_)\n",
    "#\n",
    "# # Consider Childeren == compound etc\n",
    "# noun_adj_pairs = []\n",
    "# for i,token in enumerate(doc):\n",
    "#\n",
    "#     if token.pos_ not in ('NOUN','PROPN'):\n",
    "#         continue\n",
    "#\n",
    "#     for j in range(i+1,len(doc)):\n",
    "#         if doc[j].pos_ == 'ADJ' or doc[j].pos_ == \"CCONJ\":\n",
    "#             noun_adj_pairs.append((token,doc[j]))\n",
    "#             break\n",
    "#\n",
    "# noun_adj_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parsed_sentence = nlp(u'This is my sentence')\n",
    "# [(token.text,token.i) for token in parsed_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pos = [w.replace(\"\\n\", \"\") for w in open(\"positive_lex.txt\")]\n",
    "# neg = [w.replace(\"\\n\", \"\") for w in open(\"negative_lex.txt\")]\n",
    "#\n",
    "# id = 2\n",
    "# claim =  data[id][\"titles\"] + data[id][\"arguments\"]\n",
    "# doc = nlp(claim.lower())\n",
    "#\n",
    "# pattern = nlp(keyphrases[id][0].lower())\n",
    "# print(pattern)\n",
    "#\n",
    "# compound_word = \"\"\n",
    "# for i in pattern:\n",
    "#     if i.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "#         comps = \"\".join([str(j) for j in i.children if j.dep_ == \"compound\"])\n",
    "#         if comps:\n",
    "#             compound_word = comps + \" \" + str(i)\n",
    "#\n",
    "# compound_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the targets of the given topic, claim\n",
    "# Identify the polarity (sentiment) towards each of the targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from spacy.symbols import nsubj, VERB\n",
    "#\n",
    "# doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "#\n",
    "# verbs = set()\n",
    "# for possible_subject in doc:\n",
    "#     if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "#         verbs.add(possible_subject.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# OPEN-IE\n",
    "\n",
    "# IMPORTANT NOTE; Command: java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "# TODOs: Use openie library, StanfordOpenIE\n",
    "\n",
    "# import nltk\n",
    "# from pycorenlp import *\n",
    "# import json\n",
    "# import collections\n",
    "#\n",
    "# nlp = StanfordCoreNLP(\"http://localhost:9000/\")\n",
    "# properties = {\n",
    "#     \"annotators\": \"tokenize, ssplit, pos, depparse, natlog,openie\",\n",
    "#     \"outputFormat\": \"json\",\n",
    "# }\n",
    "#\n",
    "# output = []\n",
    "# sub_pred_obj = []\n",
    "# titles = []\n",
    "\n",
    "# for j in data[13:14]:\n",
    "#     result = json.loads(nlp.annotate(j[\"titles\"], properties=properties))\n",
    "#     raw = result[\"sentences\"][0][\"openie\"]\n",
    "#     if raw:\n",
    "#         output.append([j[\"titles\"], result[\"sentences\"][0][\"openie\"]])\n",
    "#     else:\n",
    "#         continue\n",
    "#\n",
    "# # for i in output:\n",
    "# #     sub_pred_obj.append({\n",
    "# #         \"title\": i[1],\n",
    "# #         \"subject\": i[3][0][\"subject\"],\n",
    "# #         \"predicate\": i[3][0][\"relation\"],\n",
    "# #         \"object\": i[3][0][\"object\"]\n",
    "# #     })\n",
    "#\n",
    "# for i in sub_pred_obj:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'I believe that the Gay Marriage discussion isnt as important': [Gay Marriage, Marriage discussion]}]\n"
     ]
    }
   ],
   "source": [
    "# Nice Implementation\n",
    "\n",
    "# texts =  (\"I believe that the Gay Marriage discussion isnt as important\",\n",
    "#          )\n",
    "# docs = nlp.pipe(texts)\n",
    "#\n",
    "# compounds = []\n",
    "# for doc in docs:\n",
    "#     compounds.append({doc.text:[doc[tok.i:tok.head.i+1] for tok in doc if tok.dep_==\"compound\"]})\n",
    "# print(compounds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
